---
sort: 1
---



# 模型转换

* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual)

* [个人知乎](https://zhihu.com/people/zhangyj-n)

* [model_transformer](https://github.com/kg-nlp/model_transformer)  目前仓库私有

  


[TOC]

## xx2paddle
   工具x2paddle,tf适用于1.14

   [x2paddle](https://github.com/PaddlePaddle/X2Paddle)

```python
# -*- coding:utf-8 -*-
'''
# @FileName    :convert_tf2_to_paddle.py
---------------------------------
# @Time        :2023/10/8 
# @Author      :zhangyj-n
# @Email       :13091375161@163.com
# @zhihu       :https://www.zhihu.com/people/zhangyj-n
# @kg-nlp      :https://kg-nlp.github.io/Algorithm-Project-Manual/
---------------------------------
# 目标任务 :
---------------------------------
'''
import sys
import os
from loguru import logger
from tqdm import tqdm

# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from six import text_type as _text_type
from x2paddle import program
from x2paddle.utils import ConverterCheck
import argparse
import sys
from loguru import logger
import time


def arg_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument( "--model","-m",type=_text_type,default=None,help="define model file path for tensorflow or onnx")
    parser.add_argument("--prototxt", "-p",type=_text_type,default=None,help="prototxt file of caffe model")
    parser.add_argument("--weight","-w",type=_text_type,default=None,help="weight file of caffe model")
    parser.add_argument( "--save_dir","-s",type=_text_type,default=None,help="path to save translated model")
    parser.add_argument("--framework","-f",type=_text_type,default=None,help="define which deeplearning framework(tensorflow/caffe/onnx/paddle2onnx)")
    parser.add_argument("--caffe_proto","-c",type=_text_type,default=None,help="optional: the .py file compiled by caffe proto file of caffe model")
    parser.add_argument("--version", "-v", action="store_true",default=False,help="get version of x2paddle")
    parser.add_argument("--define_input_shape", "-d", action="store_true",  default=False,help="define input shape for tf model")
    parser.add_argument( "--input_shape_dict","-isd",   type=_text_type,default=None,help="define input shapes, e.g --input_shape_dict=\"{'image':[1, 3, 608, 608]}\" or" \
       "--input_shape_dict=\"{'image':[1, 3, 608, 608], 'im_shape': [1, 2], 'scale_factor': [1, 2]}\"")
    parser.add_argument("--convert_torch_project","-tp", action='store_true', help="Convert the PyTorch Project.")
    parser.add_argument("--project_dir","-pd", type=_text_type, default=None, help="define project folder path for pytorch")
    parser.add_argument( "--pretrain_model", "-pm",type=_text_type,   default=None,help="pretrain model file of pytorch model")
    parser.add_argument( "--enable_code_optim", "-co",  default=False, help="Turn on code optimization")
    parser.add_argument("--enable_onnx_checker","-oc",  default=True,help="Turn on onnx model checker")
    parser.add_argument( "--disable_feedback","-df",  default=False, help="Tune off feedback of model conversion.")
    parser.add_argument("--to_lite", "-tl", default=False, help="convert to Paddle-Lite format")
    parser.add_argument("--lite_valid_places","-vp",type=_text_type, default="arm", help="Specify the executable backend of the model")
    parser.add_argument("--lite_model_type","-mt",type=_text_type, default="naive_buffer",help="The type of lite model")
    return parser


def convert2lite(save_dir,
                 lite_valid_places="arm",
                 lite_model_type="naive_buffer"):
    """Convert to Paddle-Lite format."""

    from paddlelite.lite import Opt
    opt = Opt()
    opt.set_model_dir(save_dir + "/inference_model")
    opt.set_valid_places(lite_valid_places)
    opt.set_model_type(lite_model_type)
    opt.set_optimize_out(save_dir + "/opt")
    opt.run()


def tf2paddle(model_path,
              save_dir,
              define_input_shape=False,
              convert_to_lite=False,
              lite_valid_places="arm",
              lite_model_type="naive_buffer",
              disable_feedback=False):
    # for convert_id
    time_info = int(time.time())
    if not disable_feedback:
        ConverterCheck(
            task="TensorFlow", time_info=time_info,
            convert_state="Start").start()
    # check tensorflow installation and version
    try:
        import os
        os.environ["TF_CPP_MIN_LOG_LEVEL"] = '3'
        import tensorflow as tf
        version = tf.__version__
        if version >= '2.0.0' or version < '1.0.0':
            logger.info(
                "[ERROR] 1.0.0<=TensorFlow<2.0.0 is required, and v1.14.0 is recommended"
            )
            return
    except:
        logger.info(
            "[ERROR] TensorFlow is not installed, use \"pip install TensorFlow\"."
        )
        return

    from x2paddle.decoder.tf_decoder import TFDecoder
    from x2paddle.op_mapper.tf2paddle.tf_op_mapper import TFOpMapper

    logger.info("Now translating model from TensorFlow to Paddle.")
    model = TFDecoder(model_path, define_input_shape=define_input_shape)
    mapper = TFOpMapper(model)
    mapper.paddle_graph.build()
    logger.info("Model optimizing ...")
    from x2paddle.optimizer.optimizer import GraphOptimizer
    graph_opt = GraphOptimizer(source_frame="tf")
    graph_opt.optimize(mapper.paddle_graph)
    logger.info("Model optimized!")
    mapper.paddle_graph.gen_model(save_dir)
    logger.info("Successfully exported Paddle static graph model!")
    if not disable_feedback:
        ConverterCheck(
            task="TensorFlow", time_info=time_info,
            convert_state="Success").start()
    if convert_to_lite:
        logger.info("Now translating model from Paddle to Paddle Lite ...")
        if not disable_feedback:
            ConverterCheck(
                task="TensorFlow", time_info=time_info,
                lite_state="Start").start()
        convert2lite(save_dir, lite_valid_places, lite_model_type)
        logger.info("Successfully exported Paddle Lite support model!")
        if not disable_feedback:
            ConverterCheck(
                task="TensorFlow", time_info=time_info,
                lite_state="Success").start()
    # for convert survey
    logger.info("================================================")
    logger.info("")
    logger.info(
        "Model Converted! Fill this survey to help X2Paddle better, https://iwenjuan.baidu.com/?code=npyd51 "
    )
    logger.info("")
    logger.info("================================================")


def caffe2paddle(proto_file,
                 weight_file,
                 save_dir,
                 caffe_proto,
                 convert_to_lite=False,
                 lite_valid_places="arm",
                 lite_model_type="naive_buffer",
                 disable_feedback=False):
    # for convert_id
    time_info = int(time.time())
    if not disable_feedback:
        ConverterCheck(
            task="Caffe", time_info=time_info, convert_state="Start").start()
    from x2paddle.decoder.caffe_decoder import CaffeDecoder
    from x2paddle.op_mapper.caffe2paddle.caffe_op_mapper import CaffeOpMapper
    import google.protobuf as gpb
    ver_part = gpb.__version__.split('.')
    version_satisfy = False
    if (int(ver_part[0]) == 3 and int(ver_part[1]) >= 6) \
            or (int(ver_part[0]) > 3):
        version_satisfy = True
    assert version_satisfy, '[ERROR] google.protobuf >= 3.6.0 is required'
    logger.info("Now translating model from caffe to paddle.")
    model = CaffeDecoder(proto_file, weight_file, caffe_proto)
    mapper = CaffeOpMapper(model)
    mapper.paddle_graph.build()
    logger.info("Model optimizing ...")
    from x2paddle.optimizer.optimizer import GraphOptimizer
    graph_opt = GraphOptimizer(source_frame="caffe")
    graph_opt.optimize(mapper.paddle_graph)
    logger.info("Model optimized!")
    mapper.paddle_graph.gen_model(save_dir)
    logger.info("Successfully exported Paddle static graph model!")
    if not disable_feedback:
        ConverterCheck(
            task="Caffe", time_info=time_info, convert_state="Success").start()
    if convert_to_lite:
        logger.info("Now translating model from Paddle to Paddle Lite ...")
        if not disable_feedback:
            ConverterCheck(
                task="Caffe", time_info=time_info, lite_state="Start").start()
        convert2lite(save_dir, lite_valid_places, lite_model_type)
        logger.info("Successfully exported Paddle Lite support model!")
        if not disable_feedback:
            ConverterCheck(
                task="Caffe", time_info=time_info, lite_state="Success").start()
    # for convert survey
    logger.info("================================================")
    logger.info("")
    logger.info(
        "Model Converted! Fill this survey to help X2Paddle better, https://iwenjuan.baidu.com/?code=npyd51 "
    )
    logger.info("")
    logger.info("================================================")


def onnx2paddle(model_path,
                save_dir,
                input_shape_dict=None,
                convert_to_lite=False,
                lite_valid_places="arm",
                lite_model_type="naive_buffer",
                disable_feedback=False,
                enable_onnx_checker=True):
    # for convert_id
    time_info = int(time.time())
    if not disable_feedback:
        ConverterCheck(
            task="ONNX", time_info=time_info, convert_state="Start").start()
    # check onnx installation and version
    try:
        import onnx
        version = onnx.version.version
        v0, v1, v2 = version.split('.')
        version_sum = int(v0) * 100 + int(v1) * 10 + int(v2)
        if version_sum < 160:
            logger.info("[ERROR] onnx>=1.6.0 is required")
            return
    except:
        logger.info(
            "[ERROR] onnx is not installed, use \"pip install onnx==1.6.0\".")
        return
    logger.info("Now translating model from onnx to paddle.")

    from x2paddle.decoder.onnx_decoder import ONNXDecoder
    from x2paddle.op_mapper.onnx2paddle.onnx_op_mapper import ONNXOpMapper
    model = ONNXDecoder(model_path, enable_onnx_checker, input_shape_dict)
    mapper = ONNXOpMapper(model)
    mapper.paddle_graph.build()
    logger.info("Model optimizing ...")
    from x2paddle.optimizer.optimizer import GraphOptimizer
    graph_opt = GraphOptimizer(source_frame="onnx")
    graph_opt.optimize(mapper.paddle_graph)
    logger.info("Model optimized.")
    mapper.paddle_graph.gen_model(save_dir)
    logger.info("Successfully exported Paddle static graph model!")
    if not disable_feedback:
        ConverterCheck(
            task="ONNX", time_info=time_info, convert_state="Success").start()
    if convert_to_lite:
        logger.info("Now translating model from Paddle to Paddle Lite ...")
        if not disable_feedback:
            ConverterCheck(
                task="ONNX", time_info=time_info, lite_state="Start").start()
        convert2lite(save_dir, lite_valid_places, lite_model_type)
        logger.info("Successfully exported Paddle Lite support model!")
        if not disable_feedback:
            ConverterCheck(
                task="ONNX", time_info=time_info, lite_state="Success").start()
    # for convert survey
    logger.info("================================================")
    logger.info("")
    logger.info(
        "Model Converted! Fill this survey to help X2Paddle better, https://iwenjuan.baidu.com/?code=npyd51 "
    )
    logger.info("")
    logger.info("================================================")


def pytorch2paddle(module,
                   save_dir,
                   jit_type="trace",
                   input_examples=None,
                   enable_code_optim=False,
                   convert_to_lite=False,
                   lite_valid_places="arm",
                   lite_model_type="naive_buffer",
                   disable_feedback=False):
    # for convert_id
    time_info = int(time.time())
    if not disable_feedback:
        ConverterCheck(
            task="PyTorch", time_info=time_info, convert_state="Start").start()
    # check pytorch installation and version
    try:
        import torch
        version = torch.__version__
        v0, v1, v2 = version.split('.')
        # Avoid the situation where the version is equal to 1.7.0+cu101
        if '+' in v2:
            v2 = v2.split('+')[0]
        version_sum = int(v0) * 100 + int(v1) * 10 + int(v2)
        if version_sum < 150:
            logger.info(
                "[ERROR] PyTorch>=1.5.0 is required, 1.6.0 is the most recommended"
            )
            return
        if version_sum > 160:
            logger.info("[WARNING] PyTorch==1.6.0 is recommended")
    except:
        logger.info(
            "[ERROR] PyTorch is not installed, use \"pip install torch==1.6.0 torchvision\"."
        )
        return
    logger.info("Now translating model from PyTorch to Paddle.")

    from x2paddle.decoder.pytorch_decoder import ScriptDecoder, TraceDecoder
    from x2paddle.op_mapper.pytorch2paddle.pytorch_op_mapper import PyTorchOpMapper

    if jit_type == "trace":
        model = TraceDecoder(module, input_examples)
    else:
        model = ScriptDecoder(module, input_examples)
    mapper = PyTorchOpMapper(model)
    mapper.paddle_graph.build()
    logger.info("Model optimizing ...")
    from x2paddle.optimizer.optimizer import GraphOptimizer
    graph_opt = GraphOptimizer(source_frame="pytorch", jit_type=jit_type)
    graph_opt.optimize(mapper.paddle_graph)
    logger.info("Model optimized!")
    mapper.paddle_graph.gen_model(
        save_dir, jit_type=jit_type, enable_code_optim=enable_code_optim)
    logger.info("Successfully exported Paddle static graph model!")
    if not disable_feedback:
        ConverterCheck(
            task="PyTorch", time_info=time_info,
            convert_state="Success").start()
    if convert_to_lite:
        logger.info("Now translating model from Paddle to Paddle Lite ...")
        if not disable_feedback:
            ConverterCheck(
                task="PyTorch", time_info=time_info, lite_state="Start").start()
        convert2lite(save_dir, lite_valid_places, lite_model_type)
        logger.info("Successfully exported Paddle Lite support model!")
        if not disable_feedback:
            ConverterCheck(
                task="PyTorch", time_info=time_info,
                lite_state="Success").start()
    # for convert survey
    logger.info("================================================")
    logger.info("")
    logger.info(
        "Model Converted! Fill this survey to help X2Paddle better, https://iwenjuan.baidu.com/?code=npyd51 "
    )
    logger.info("")
    logger.info("================================================")


def main():
    if len(sys.argv) < 2:
        logger.info("Use \"x2paddle -h\" to print the help information")
        logger.info(
            "For more information, please follow our github repo below:)")
        logger.info("\nGithub: https://github.com/PaddlePaddle/X2Paddle.git\n")
        return

    parser = arg_parser()
    args = parser.parse_args()

    if args.version:
        import x2paddle
        logger.info("x2paddle-{} with python>=3.5, paddlepaddle>=1.6.0\n".
                     format(x2paddle.__version__))
        return

    if not args.convert_torch_project:
        assert args.framework is not None, "--framework is not defined(support tensorflow/caffe/onnx)"
    assert args.save_dir is not None, "--save_dir is not defined"

    try:
        import platform
        v0, v1, v2 = platform.python_version().split('.')
        if not (int(v0) >= 3 and int(v1) >= 5):
            logger.info("[ERROR] python>=3.5 is required")
            return
        import paddle
        v0, v1, v2 = paddle.__version__.split('.')
        logger.info("paddle.__version__ = {}".format(paddle.__version__))
        if v0 == '0' and v1 == '0' and v2 == '0':
            logger.info(
                "[WARNING] You are use develop version of paddlepaddle")
        elif int(v0) != 2 or int(v1) < 0:
            logger.info("[ERROR] paddlepaddle>=2.0.0 is required")
            return
    except:
        logger.info(
            "[ERROR] paddlepaddle not installed, use \"pip install paddlepaddle\""
        )

    if args.convert_torch_project:
        assert args.project_dir is not None, "--project_dir should be defined while translating pytorch project"
        from x2paddle.project_convertor.pytorch.convert import main as convert_torch
        convert_torch(args)
    else:
        if args.framework == "tensorflow":
            assert args.model is not None, "--model should be defined while translating tensorflow model"
            define_input_shape = False
            if args.define_input_shape:
                define_input_shape = True
            tf2paddle(
                args.model,
                args.save_dir,
                define_input_shape,
                convert_to_lite=args.to_lite,
                lite_valid_places=args.lite_valid_places,
                lite_model_type=args.lite_model_type,
                disable_feedback=args.disable_feedback)

        elif args.framework == "caffe":
            assert args.prototxt is not None and args.weight is not None, "--prototxt and --weight should be defined while translating caffe model"
            caffe2paddle(
                args.prototxt,
                args.weight,
                args.save_dir,
                args.caffe_proto,
                convert_to_lite=args.to_lite,
                lite_valid_places=args.lite_valid_places,
                lite_model_type=args.lite_model_type,
                disable_feedback=args.disable_feedback)
        elif args.framework == "onnx":
            assert args.model is not None, "--model should be defined while translating onnx model"
            onnx2paddle(
                args.model,
                args.save_dir,
                input_shape_dict=args.input_shape_dict,
                convert_to_lite=args.to_lite,
                lite_valid_places=args.lite_valid_places,
                lite_model_type=args.lite_model_type,
                disable_feedback=args.disable_feedback,
                enable_onnx_checker=args.enable_onnx_checker)
        elif args.framework == "paddle2onnx":
            logger.info(
                "Paddle to ONNX tool has been migrated to the new github: https://github.com/PaddlePaddle/paddle2onnx"
            )

        else:
            raise Exception(
                "--framework only support tensorflow/caffe/onnx now")


if __name__ == "__main__":
    main()




```



## torch2paddle

[解读 Bert 模型权重转换](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/model_convert/convert_from_pytorch/convert_bert_weights_cn.html)

* 加载torch版的ernie

```python
from transformers import ErnieModel as TorchErnieModel
torch_model = TorchErnieModel.from_pretrained('/workspace/custom_project/pretrain_model_file/Ernie-Torch-model/ernie-3.0-base-zh')  # 加载torch版的huggingface的ernie结构
for name,param in torch_model.named_parameters():
    print(name)
    print(list(param.data.shape))
    print(name, list(param.data.shape))
   
# embeddings.word_embeddings.weight
# embeddings.position_embeddings.weight
# embeddings.token_type_embeddings.weight
# embeddings.task_type_embeddings.weight
# embeddings.LayerNorm.weight
# embeddings.LayerNorm.bias
# encoder.layer.{}.attention.self.query.weight
# encoder.layer.{}.attention.self.query.bias
# encoder.layer.{}.attention.self.key.weight
# encoder.layer.{}.attention.self.key.bias
# encoder.layer.{}.attention.self.value.weight
# encoder.layer.{}.attention.self.value.bias
# encoder.layer.{}.attention.output.dense.weight
# encoder.layer.{}.attention.output.dense.bias
# encoder.layer.{}.attention.output.LayerNorm.weight
# encoder.layer.{}.attention.output.LayerNorm.bias
# encoder.layer.{}.intermediate.dense.weight
# encoder.layer.{}.intermediate.dense.bias
# encoder.layer.{}.output.dense.weight
# encoder.layer.{}.output.dense.bias
# encoder.layer.{}.output.LayerNorm.weight
# encoder.layer.{}.output.LayerNorm.bias
# pooler.dense.weight  # 最后一层 encoder 输出的池化操作
# pooler.dense.bias



'''使用预训练模型加载方式和torch直接加载方式 模型参数名称叫法不同'''
pytorch_state_dict = torch.load('/workspace/custom_project/pretrain_model_file/Ernie-Torch-model/ernie-3.0-base-zh/pytorch_model.bin')  # 加载torch版的模型权重名称
for key in pytorch_state_dict.keys():
    print(key)
    # roberta.embeddings.position_ids
    # roberta.embeddings.word_embeddings.weight
    # roberta.embeddings.position_embeddings.weight
    # roberta.embeddings.token_type_embeddings.weight
    # roberta.embeddings.LayerNorm.gamma
    # roberta.embeddings.LayerNorm.beta
    # roberta.encoder.layer.{}.attention.self.query.weight
    # roberta.encoder.layer.{}.attention.self.query.bias
    # roberta.encoder.layer.{}.attention.self.key.weight
    # roberta.encoder.layer.{}.attention.self.key.bias
    # roberta.encoder.layer.{}.attention.self.value.weight
    # roberta.encoder.layer.{}.attention.self.value.bias
    # roberta.encoder.layer.{}.attention.output.dense.weight
    # roberta.encoder.layer.{}.attention.output.dense.bias
    # roberta.encoder.layer.{}.attention.output.LayerNorm.gamma
    # roberta.encoder.layer.{}.attention.output.LayerNorm.beta
    # roberta.encoder.layer.{}.intermediate.dense.weight
    # roberta.encoder.layer.{}.intermediate.dense.bias
    # roberta.encoder.layer.{}.output.dense.weight
    # roberta.encoder.layer.{}.output.dense.bias
    # roberta.encoder.layer.{}.output.LayerNorm.gamma
    # roberta.encoder.layer.{}.output.LayerNorm.beta
    # pooler.dense.weight  # 最后一层 encoder 输出的池化操作
    # pooler.dense.bias
```

* 加载paddle版的ernie

```python
from paddlenlp.transformers import ErnieModel as PaddleErnieModel
paddle_model = PaddleErnieModel.from_pretrained('/workspace/custom_project/pretrain_model_file/Ernie-model/ernie-3.0-base-zh')
# print(paddle_model)
for name,param in paddle_model.named_parameters():
    # print("|","|","|",name,'|',param.shape,"|")
    # print(name)
    print(param.shape)
    
# embeddings.word_embeddings.weight
# embeddings.position_embeddings.weight
# embeddings.token_type_embeddings.weight
# embeddings.task_type_embeddings.weight
# embeddings.layer_norm.weight
# embeddings.layer_norm.bias
# encoder.layers.0.self_attn.q_proj.weight
# encoder.layers.0.self_attn.q_proj.bias
# encoder.layers.0.self_attn.k_proj.weight
# encoder.layers.0.self_attn.k_proj.bias
# encoder.layers.0.self_attn.v_proj.weight
# encoder.layers.0.self_attn.v_proj.bias
# encoder.layers.0.self_attn.out_proj.weight
# encoder.layers.0.self_attn.out_proj.bias
# encoder.layers.0.linear1.weight
# encoder.layers.0.linear1.bias
# encoder.layers.0.linear2.weight
# encoder.layers.0.linear2.bias
# encoder.layers.0.norm1.weight
# encoder.layers.0.norm1.bias
# encoder.layers.0.norm2.weight
# encoder.layers.0.norm2.bias
# pooler.dense.weight
# pooler.dense.bias
```





* 基于pytorch加载和基于paddle加载模型static_dict对应关系



* 建立keys和values的对应关系
* torch的ernie结构
```python
from transformers import ErnieModel as TorchErnieModel
# 通过下面tab缩进表示模型结构

self.embeddings = ErnieEmbeddings(config)
	self.word_embeddings = nn.Embedding(config.vocab_size, )
    self.position_embeddings = nn.Embedding(config.max_position_embeddings, )
    self.token_type_embeddings = nn.Embedding(config.type_vocab_size, )
    self.task_type_embeddings = nn.Embedding(config.task_type_vocab_size, )
    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
self.encoder = ErnieEncoder(config)
	self.layer = nn.ModuleList([ErnieLayer(config)
        self.attention = ErnieAttention(config)
            self.self = ErnieSelfAttention(config, position_embedding_type)
                self.query = nn.Linear(config.hidden_size, self.all_head_size)
                self.key = nn.Linear(config.hidden_size, self.all_head_size)
                self.value = nn.Linear(config.hidden_size, self.all_head_size)
            self.output = ErnieSelfOutput(config)
                self.dense = nn.Linear(config.hidden_size, config.hidden_size)
                self.LayerNorm = nn.LayerNorm(config.hidden_size, eps)
        self.intermediate = ErnieIntermediate(config)
        	self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        self.output = ErnieOutput(config)
        	self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        	self.LayerNorm = nn.LayerNorm(config.hidden_size, eps)
self.pooler = ErniePooler(config) if add_pooling_layer else None
	self.dense = nn.Linear(config.hidden_size, config.hidden_size)
    
    
torch_model = TorchErnieModel.from_pretrained('/workspace/custom_project/pretrain_model_file/Ernie-Torch-model/ernie-3.0-base-zh')  # 加载torch版的huggingface的ernie结构
    print(torch_model)
    
    
'''
ErnieModel(
   (embeddings): ErnieEmbeddings(
        (word_embeddings): Embedding(40000, 768, padding_idx=0)
        (position_embeddings): Embedding(2048, 768)
        (token_type_embeddings): Embedding(4, 768)
        (task_type_embeddings): Embedding(3, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
    (encoder): ErnieEncoder(
        (layer): ModuleList(
          (0): ErnieLayer(
            (attention): ErnieAttention(
              (self): ErnieSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
              )
              (output): ErnieSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
            )
            (intermediate): ErnieIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): ErnieOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          ...
	(pooler): ErniePooler(
    	(dense): Linear(in_features=768, out_features=768, bias=True)
  )   
)
'''
```

  * paddle的ernie结构

```Python
from paddlenlp.transformers import ErnieModel as PaddleErnieModel
# 通过下面tab缩进表示模型结构

self.embeddings = ErnieEmbeddings(config=config, weight_attr=weight_attr)
	self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx, weight_attr)
    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size,   )
    self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size, weight_attr)
    self.task_type_embeddings = nn.Embedding(config.task_type_vocab_size, config.hidden_size, )
   	self.layer_norm = nn.LayerNorm(config.hidden_size)
self.encoder = nn.TransformerEncoder(encoder_layer, config.num_hidden_layers,enable_recompute)
	 self.layers = LayerList()
    	self.self_attn = MultiHeadAttention()
        	self.q_proj = Linear(embed_dim,embed_dim,weight_attr,bias_attr=bias_attr)
            self.k_proj = Linear(self.kdim,embed_dim,weight_attr,bias_attr=bias_attr)
            self.v_proj = Linear(self.vdim,embed_dim,weight_attr,bias_attr=bias_attr)
            self.out_proj = Linear(embed_dim,embed_dim,weight_attr, bias_attr=bias_attr)
        self.linear1 = Linear(d_model,dim_feedforward,)
        self.linear2 = Linear(dim_feedforward,d_model,)
        self.norm1 = LayerNorm(d_model)
        self.norm2 = LayerNorm(d_model)
self.pooler = ErniePooler(config, weight_attr)
	self.dense = nn.Linear(config.hidden_size, config.hidden_size, weight_attr=weight_attr)

    
paddle_model = PaddleErnieModel.from_pretrained('/workspace/custom_project/pretrain_model_file/Ernie-model/ernie-3.0-base-zh')
    print(paddle_model)    
 
'''
ErnieModel(
  (embeddings): ErnieEmbeddings(
    (word_embeddings): Embedding(40000, 768, padding_idx=0, sparse=False)
    (position_embeddings): Embedding(2048, 768, sparse=False)
    (token_type_embeddings): Embedding(4, 768, sparse=False)
    (task_type_embeddings): Embedding(3, 768, sparse=False)
    (layer_norm): LayerNorm(normalized_shape=[768], epsilon=1e-12)
  )
  (encoder): TransformerEncoder(
    (layers): LayerList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiHeadAttention(
          (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
          (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
          (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
          (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
        )
        (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
        (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
        (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)
        (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)
      )
 (pooler): ErniePooler(
    (dense): Linear(in_features=768, out_features=768, dtype=float32)
  )
)
'''
```



* 通过对上述结构,参数形状,修改对应关系

```Python
'''torch转paddle模型'''
# 不知道模型结构的时候,用torch.load 加载查看模型结构,参数,形状
pytorch_state_dict = torch.load('/workspace/custom_project/pretrain_model_file/Ernie-Torch-model/ernie-3.0-base-zh/pytorch_model.bin')  # 加载torch版的模型权重名称
paddle_state_dict = paddle.load('/workspace/custom_project/pretrain_model_file/Ernie-model/ernie-3.0-base-zh/model_state.pdparams')

paddle_model_path = "./convert_torch_to_paddle/ernie_3.0_torch/ernie_torch2paddle.pdparams"
# State_dict's keys mapping: from torch to paddle
weight_map = collections.OrderedDict({
    # embedding
    'ernie.embeddings.word_embeddings.weight': "ernie.embeddings.word_embeddings.weight",
    'ernie.embeddings.position_embeddings.weight': "ernie.embeddings.position_embeddings.weight",
    'ernie.embeddings.token_type_embeddings.weight': "ernie.embeddings.token_type_embeddings.weight",
    'ernie.embeddings.task_type_embeddings.weight': "ernie.embeddings.task_type_embeddings.weight",
    'ernie.embeddings.LayerNorm.gamma': 'ernie.embeddings.layer_norm.weight',
    'ernie.embeddings.LayerNorm.beta': 'ernie.embeddings.layer_norm.bias',})
# encoder layer
# add attention layers
for i in range(12):
    weight_map[f'ernie.encoder.layer.{i}.attention.self.query.weight'] = f'ernie.encoder.layers.{i}.self_attn.q_proj.weight'
    weight_map[f'ernie.encoder.layer.{i}.attention.self.query.bias'] = f'ernie.encoder.layers.{i}.self_attn.q_proj.bias'
    weight_map[f'ernie.encoder.layer.{i}.attention.self.key.weight'] = f'ernie.encoder.layers.{i}.self_attn.k_proj.weight'
    weight_map[f'ernie.encoder.layer.{i}.attention.self.key.bias'] = f'ernie.encoder.layers.{i}.self_attn.k_proj.bias'
    weight_map[f'ernie.encoder.layer.{i}.attention.self.value.weight'] = f'ernie.encoder.layers.{i}.self_attn.v_proj.weight'
    weight_map[f'ernie.encoder.layer.{i}.attention.self.value.bias'] = f'ernie.encoder.layers.{i}.self_attn.v_proj.bias'
    weight_map[f'ernie.encoder.layer.{i}.attention.output.dense.weight'] = f'ernie.encoder.layers.{i}.self_attn.out_proj.weight'
    weight_map[f'ernie.encoder.layer.{i}.attention.output.dense.bias'] = f'ernie.encoder.layers.{i}.self_attn.out_proj.bias'
    weight_map[f'ernie.encoder.layer.{i}.attention.output.LayerNorm.gamma'] = f'ernie.encoder.layers.{i}.norm1.weight'
    weight_map[f'ernie.encoder.layer.{i}.attention.output.LayerNorm.beta'] = f'ernie.encoder.layers.{i}.norm1.bias'
    weight_map[f'ernie.encoder.layer.{i}.intermediate.dense.weight'] = f'ernie.encoder.layers.{i}.linear1.weight'
    weight_map[f'ernie.encoder.layer.{i}.intermediate.dense.bias'] = f'ernie.encoder.layers.{i}.linear1.bias'
    weight_map[f'ernie.encoder.layer.{i}.output.dense.weight'] = f'ernie.encoder.layers.{i}.linear2.weight'
    weight_map[f'ernie.encoder.layer.{i}.output.dense.bias'] = f'ernie.encoder.layers.{i}.linear2.bias'
    weight_map[f'ernie.encoder.layer.{i}.output.LayerNorm.gamma'] = f'ernie.encoder.layers.{i}.norm2.weight'
    weight_map[f'ernie.encoder.layer.{i}.output.LayerNorm.beta'] = f'ernie.encoder.layers.{i}.norm2.bias'

weight_map.update(
        {
            'ernie.pooler.dense.weight': 'ernie.pooler.dense.weight',
            'ernie.pooler.dense.bias': 'ernie.pooler.dense.bias',
            'cls.predictions.transform.dense.weight': 'cls.predictions.transform.weight',  # 这几个在编码的时候不需要(预测)
            'cls.predictions.transform.dense.bias': 'cls.predictions.transform.bias',
            'cls.predictions.transform.LayerNorm.gamma':'cls.predictions.layer_norm.weight',
            'cls.predictions.transform.LayerNorm.beta':'cls.predictions.layer_norm.bias',
            'cls.predictions.bias':'cls.predictions.decoder_bias',
            'cls.predictions.weight': 'cls.predictions.decoder_weight',
        }
    )

'''查看torch和paddle结构的形状'''
for k,v in pytorch_state_dict.items():
    print("|",k,"|",list(v.data.shape),"|",weight_map[k],"|",paddle_state_dict[weight_map[k]].shape,"|")
```



【金山文档】 模型转换 https://kdocs.cn/l/cv8mdEWTGBBi    

两者的顺序是基本一致的，但也有一些例外，例如：

- `ernie.encoder.layers.0.norm1.weight` 对应 `ernie.encoder.layer.0.attention.output.LayerNorm.gamma` ；
- `ernie.encoder.layers.0.norm1.bias` 对应 `ernie.encoder.layer.0.attention.output.LayerNorm.beta` ；
- `ernie.encoder.layer.0.intermediate.dense.weight` 对应 `ernie.encoder.layers.0.linear1.weight` ；
- `ernie.encoder.layer.0.output.dense.weight` 对应 `ernie.encoder.layers.0.linear2.weight` ；
- `ernie.encoder.layer.0.output.LayerNorm.gamma` 对应 `ernie.encoder.layers.0.norm2.weight`。

另外还需要注意其他一些细节，这里列出来几个可能会遇到的情景以供参考：

- 有些模型结构可能在实现时对参数的处理有差异导致存在参数的拆分或者合并等操作， 此时我们需要进行参数多对一或者一对多的映射，同时将对应的 values 拆分或者合并。
- 还有存在 batch norm 层时，我们需要注意 todo。


|keys(torch)|	shape(torch)|	keys(paddle)|	shape(paddle)|
|---|---|---|---|
| ernie.encoder.layer.9.output.dense.weight | [768, 3072] | ernie.encoder.layers.9.linear2.weight | [3072, 768] |
| cls.predictions.transform.dense.weight | [768, 768] | cls.predictions.transform.weight | [768, 768] |
| ernie.encoder.layer.6.intermediate.dense.bias | [3072] | ernie.encoder.layers.6.linear1.bias | [3072] |
| ernie.encoder.layer.6.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.6.linear1.weight | [768, 3072] |
| ernie.encoder.layer.10.attention.output.dense.bias | [768] | ernie.encoder.layers.10.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.6.output.dense.weight | [768, 3072] | ernie.encoder.layers.6.linear2.weight | [3072, 768] |
| ernie.encoder.layer.5.output.dense.weight | [768, 3072] | ernie.encoder.layers.5.linear2.weight | [3072, 768] |
| ernie.encoder.layer.11.attention.self.query.weight | [768, 768] | ernie.encoder.layers.11.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.0.output.LayerNorm.beta | [768] | ernie.encoder.layers.0.norm2.bias | [768] |
| ernie.encoder.layer.2.attention.self.key.weight | [768, 768] | ernie.encoder.layers.2.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.6.attention.output.dense.bias | [768] | ernie.encoder.layers.6.self_attn.out_proj.bias | [768] |
| ernie.embeddings.task_type_embeddings.weight | [3, 768] | ernie.embeddings.task_type_embeddings.weight | [3, 768] |
| ernie.encoder.layer.7.attention.self.key.weight | [768, 768] | ernie.encoder.layers.7.self_attn.k_proj.weight | [768, 768] |
| cls.predictions.transform.LayerNorm.gamma | [768] | cls.predictions.layer_norm.weight | [768] |
| ernie.encoder.layer.3.output.dense.weight | [768, 3072] | ernie.encoder.layers.3.linear2.weight | [3072, 768] |
| ernie.encoder.layer.1.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.1.linear1.weight | [768, 3072] |
| ernie.encoder.layer.0.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.0.linear1.weight | [768, 3072] |
| ernie.encoder.layer.9.attention.self.key.bias | [768] | ernie.encoder.layers.9.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.7.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.7.norm1.bias | [768] |
| ernie.encoder.layer.0.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.0.norm1.weight | [768] |
| ernie.encoder.layer.0.attention.self.value.weight | [768, 768] | ernie.encoder.layers.0.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.1.output.LayerNorm.beta | [768] | ernie.encoder.layers.1.norm2.bias | [768] |
| ernie.encoder.layer.1.attention.output.dense.bias | [768] | ernie.encoder.layers.1.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.4.intermediate.dense.bias | [3072] | ernie.encoder.layers.4.linear1.bias | [3072] |
| ernie.embeddings.LayerNorm.gamma | [768] | ernie.embeddings.layer_norm.weight | [768] |
| ernie.encoder.layer.6.attention.self.value.bias | [768] | ernie.encoder.layers.6.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.7.output.dense.bias | [768] | ernie.encoder.layers.7.linear2.bias | [768] |
| ernie.encoder.layer.0.intermediate.dense.bias | [3072] | ernie.encoder.layers.0.linear1.bias | [3072] |
| ernie.encoder.layer.4.attention.output.dense.bias | [768] | ernie.encoder.layers.4.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.1.attention.self.key.weight | [768, 768] | ernie.encoder.layers.1.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.7.attention.self.value.weight | [768, 768] | ernie.encoder.layers.7.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.2.output.dense.bias | [768] | ernie.encoder.layers.2.linear2.bias | [768] |
| ernie.encoder.layer.11.intermediate.dense.bias | [3072] | ernie.encoder.layers.11.linear1.bias | [3072] |
| ernie.encoder.layer.7.attention.self.query.bias | [768] | ernie.encoder.layers.7.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.1.attention.self.key.bias | [768] | ernie.encoder.layers.1.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.2.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.2.linear1.weight | [768, 3072] |
| ernie.encoder.layer.5.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.5.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.0.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.0.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.8.attention.self.key.weight | [768, 768] | ernie.encoder.layers.8.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.7.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.7.norm1.weight | [768] |
| ernie.encoder.layer.1.intermediate.dense.bias | [3072] | ernie.encoder.layers.1.linear1.bias | [3072] |
| ernie.encoder.layer.5.attention.self.value.weight | [768, 768] | ernie.encoder.layers.5.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.2.attention.self.key.bias | [768] | ernie.encoder.layers.2.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.5.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.5.linear1.weight | [768, 3072] |
| ernie.encoder.layer.4.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.4.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.10.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.10.linear1.weight | [768, 3072] |
| ernie.encoder.layer.8.output.dense.bias | [768] | ernie.encoder.layers.8.linear2.bias | [768] |
| ernie.encoder.layer.6.attention.self.key.weight | [768, 768] | ernie.encoder.layers.6.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.6.attention.self.key.bias | [768] | ernie.encoder.layers.6.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.1.attention.self.query.bias | [768] | ernie.encoder.layers.1.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.4.output.dense.bias | [768] | ernie.encoder.layers.4.linear2.bias | [768] |
| ernie.encoder.layer.1.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.1.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.1.output.dense.bias | [768] | ernie.encoder.layers.1.linear2.bias | [768] |
| ernie.encoder.layer.10.attention.self.value.bias | [768] | ernie.encoder.layers.10.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.8.attention.self.query.weight | [768, 768] | ernie.encoder.layers.8.self_attn.q_proj.weight | [768, 768] |
| ernie.pooler.dense.bias | [768] | ernie.pooler.dense.bias | [768] |
| ernie.encoder.layer.8.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.8.norm1.bias | [768] |
| ernie.encoder.layer.4.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.4.norm1.bias | [768] |
| ernie.encoder.layer.3.output.dense.bias | [768] | ernie.encoder.layers.3.linear2.bias | [768] |
| ernie.encoder.layer.3.output.LayerNorm.gamma | [768] | ernie.encoder.layers.3.norm2.weight | [768] |
| ernie.encoder.layer.10.attention.self.query.weight | [768, 768] | ernie.encoder.layers.10.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.10.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.10.norm1.weight | [768] |
| ernie.encoder.layer.0.attention.output.dense.bias | [768] | ernie.encoder.layers.0.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.2.output.LayerNorm.beta | [768] | ernie.encoder.layers.2.norm2.bias | [768] |
| ernie.encoder.layer.6.output.LayerNorm.gamma | [768] | ernie.encoder.layers.6.norm2.weight | [768] |
| ernie.encoder.layer.8.output.LayerNorm.gamma | [768] | ernie.encoder.layers.8.norm2.weight | [768] |
| ernie.encoder.layer.11.attention.self.value.weight | [768, 768] | ernie.encoder.layers.11.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.8.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.8.norm1.weight | [768] |
| ernie.encoder.layer.8.attention.output.dense.bias | [768] | ernie.encoder.layers.8.self_attn.out_proj.bias | [768] |
| ernie.pooler.dense.weight | [768, 768] | ernie.pooler.dense.weight | [768, 768] |
| ernie.encoder.layer.4.attention.self.key.bias | [768] | ernie.encoder.layers.4.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.7.output.dense.weight | [768, 3072] | ernie.encoder.layers.7.linear2.weight | [3072, 768] |
| ernie.encoder.layer.11.output.LayerNorm.gamma | [768] | ernie.encoder.layers.11.norm2.weight | [768] |
| ernie.encoder.layer.2.attention.self.value.weight | [768, 768] | ernie.encoder.layers.2.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.11.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.11.norm1.bias | [768] |
| ernie.encoder.layer.6.attention.self.value.weight | [768, 768] | ernie.encoder.layers.6.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.10.attention.self.value.weight | [768, 768] | ernie.encoder.layers.10.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.9.attention.self.value.weight | [768, 768] | ernie.encoder.layers.9.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.11.attention.output.dense.bias | [768] | ernie.encoder.layers.11.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.8.output.LayerNorm.beta | [768] | ernie.encoder.layers.8.norm2.bias | [768] |
| ernie.encoder.layer.3.attention.self.value.bias | [768] | ernie.encoder.layers.3.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.8.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.8.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.10.output.dense.weight | [768, 3072] | ernie.encoder.layers.10.linear2.weight | [3072, 768] |
| ernie.encoder.layer.7.attention.self.query.weight | [768, 768] | ernie.encoder.layers.7.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.9.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.9.norm1.bias | [768] |
| ernie.encoder.layer.1.attention.self.value.weight | [768, 768] | ernie.encoder.layers.1.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.10.attention.self.key.bias | [768] | ernie.encoder.layers.10.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.6.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.6.norm1.weight | [768] |
| ernie.encoder.layer.3.attention.self.query.bias | [768] | ernie.encoder.layers.3.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.5.attention.self.key.weight | [768, 768] | ernie.encoder.layers.5.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.9.attention.self.value.bias | [768] | ernie.encoder.layers.9.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.7.intermediate.dense.bias | [3072] | ernie.encoder.layers.7.linear1.bias | [3072] |
| ernie.encoder.layer.11.attention.self.value.bias | [768] | ernie.encoder.layers.11.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.5.attention.output.dense.bias | [768] | ernie.encoder.layers.5.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.10.output.LayerNorm.gamma | [768] | ernie.encoder.layers.10.norm2.weight | [768] |
| ernie.encoder.layer.9.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.9.linear1.weight | [768, 3072] |
| ernie.encoder.layer.7.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.7.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.3.intermediate.dense.bias | [3072] | ernie.encoder.layers.3.linear1.bias | [3072] |
| ernie.encoder.layer.1.output.dense.weight | [768, 3072] | ernie.encoder.layers.1.linear2.weight | [3072, 768] |
| ernie.encoder.layer.0.attention.self.key.bias | [768] | ernie.encoder.layers.0.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.5.output.LayerNorm.gamma | [768] | ernie.encoder.layers.5.norm2.weight | [768] |
| ernie.encoder.layer.2.attention.self.value.bias | [768] | ernie.encoder.layers.2.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.0.attention.self.key.weight | [768, 768] | ernie.encoder.layers.0.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.5.output.dense.bias | [768] | ernie.encoder.layers.5.linear2.bias | [768] |
| cls.predictions.transform.dense.bias | [768] | cls.predictions.transform.bias | [768] |
| ernie.encoder.layer.11.attention.self.query.bias | [768] | ernie.encoder.layers.11.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.0.output.dense.weight | [768, 3072] | ernie.encoder.layers.0.linear2.weight | [3072, 768] |
| ernie.encoder.layer.11.output.LayerNorm.beta | [768] | ernie.encoder.layers.11.norm2.bias | [768] |
| ernie.encoder.layer.6.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.6.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.4.attention.self.query.weight | [768, 768] | ernie.encoder.layers.4.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.1.attention.self.query.weight | [768, 768] | ernie.encoder.layers.1.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.8.attention.self.value.weight | [768, 768] | ernie.encoder.layers.8.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.7.attention.self.key.bias | [768] | ernie.encoder.layers.7.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.5.attention.self.query.bias | [768] | ernie.encoder.layers.5.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.11.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.11.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.3.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.3.self_attn.out_proj.weight | [768, 768] |
| ernie.embeddings.LayerNorm.beta | [768] | ernie.embeddings.layer_norm.bias | [768] |
| ernie.encoder.layer.5.attention.self.query.weight | [768, 768] | ernie.encoder.layers.5.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.10.attention.self.key.weight | [768, 768] | ernie.encoder.layers.10.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.8.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.8.linear1.weight | [768, 3072] |
| ernie.encoder.layer.9.attention.self.key.weight | [768, 768] | ernie.encoder.layers.9.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.11.attention.self.key.bias | [768] | ernie.encoder.layers.11.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.7.attention.output.dense.bias | [768] | ernie.encoder.layers.7.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.10.output.LayerNorm.beta | [768] | ernie.encoder.layers.10.norm2.bias | [768] |
| ernie.embeddings.position_embeddings.weight | [2048, 768] | ernie.embeddings.position_embeddings.weight | [2048, 768] |
| ernie.encoder.layer.5.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.5.norm1.weight | [768] |
| ernie.encoder.layer.3.attention.self.key.bias | [768] | ernie.encoder.layers.3.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.3.attention.self.key.weight | [768, 768] | ernie.encoder.layers.3.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.3.output.LayerNorm.beta | [768] | ernie.encoder.layers.3.norm2.bias | [768] |
| ernie.encoder.layer.11.attention.self.key.weight | [768, 768] | ernie.encoder.layers.11.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.5.attention.self.key.bias | [768] | ernie.encoder.layers.5.self_attn.k_proj.bias | [768] |
| ernie.encoder.layer.7.attention.self.value.bias | [768] | ernie.encoder.layers.7.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.3.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.3.linear1.weight | [768, 3072] |
| ernie.encoder.layer.2.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.2.norm1.bias | [768] |
| ernie.encoder.layer.6.output.LayerNorm.beta | [768] | ernie.encoder.layers.6.norm2.bias | [768] |
| ernie.encoder.layer.11.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.11.linear1.weight | [768, 3072] |
| ernie.encoder.layer.4.attention.self.value.bias | [768] | ernie.encoder.layers.4.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.2.attention.self.query.weight | [768, 768] | ernie.encoder.layers.2.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.4.output.LayerNorm.gamma | [768] | ernie.encoder.layers.4.norm2.weight | [768] |
| ernie.encoder.layer.2.output.LayerNorm.gamma | [768] | ernie.encoder.layers.2.norm2.weight | [768] |
| ernie.encoder.layer.2.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.2.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.1.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.1.norm1.weight | [768] |
| ernie.encoder.layer.4.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.4.norm1.weight | [768] |
| ernie.encoder.layer.3.attention.self.value.weight | [768, 768] | ernie.encoder.layers.3.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.11.output.dense.bias | [768] | ernie.encoder.layers.11.linear2.bias | [768] |
| ernie.encoder.layer.5.intermediate.dense.bias | [3072] | ernie.encoder.layers.5.linear1.bias | [3072] |
| ernie.encoder.layer.2.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.2.norm1.weight | [768] |
| ernie.embeddings.word_embeddings.weight | [40000, 768] | ernie.embeddings.word_embeddings.weight | [40000, 768] |
| ernie.encoder.layer.0.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.0.norm1.bias | [768] |
| ernie.encoder.layer.9.attention.output.dense.bias | [768] | ernie.encoder.layers.9.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.10.intermediate.dense.bias | [3072] | ernie.encoder.layers.10.linear1.bias | [3072] |
| ernie.encoder.layer.9.output.LayerNorm.beta | [768] | ernie.encoder.layers.9.norm2.bias | [768] |
| ernie.encoder.layer.2.attention.output.dense.bias | [768] | ernie.encoder.layers.2.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.4.attention.self.value.weight | [768, 768] | ernie.encoder.layers.4.self_attn.v_proj.weight | [768, 768] |
| ernie.encoder.layer.5.output.LayerNorm.beta | [768] | ernie.encoder.layers.5.norm2.bias | [768] |
| ernie.encoder.layer.5.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.5.norm1.bias | [768] |
| ernie.embeddings.token_type_embeddings.weight | [4, 768] | ernie.embeddings.token_type_embeddings.weight | [4, 768] |
| ernie.encoder.layer.2.attention.self.query.bias | [768] | ernie.encoder.layers.2.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.4.output.dense.weight | [768, 3072] | ernie.encoder.layers.4.linear2.weight | [3072, 768] |
| ernie.encoder.layer.5.attention.self.value.bias | [768] | ernie.encoder.layers.5.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.10.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.10.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.8.attention.self.query.bias | [768] | ernie.encoder.layers.8.self_attn.q_proj.bias | [768] |
| cls.predictions.transform.LayerNorm.beta | [768] | cls.predictions.layer_norm.bias | [768] |
| ernie.encoder.layer.4.attention.self.key.weight | [768, 768] | ernie.encoder.layers.4.self_attn.k_proj.weight | [768, 768] |
| ernie.encoder.layer.7.output.LayerNorm.beta | [768] | ernie.encoder.layers.7.norm2.bias | [768] |
| ernie.encoder.layer.0.output.LayerNorm.gamma | [768] | ernie.encoder.layers.0.norm2.weight | [768] |
| ernie.encoder.layer.0.attention.self.query.weight | [768, 768] | ernie.encoder.layers.0.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.1.output.LayerNorm.gamma | [768] | ernie.encoder.layers.1.norm2.weight | [768] |
| ernie.encoder.layer.8.attention.self.value.bias | [768] | ernie.encoder.layers.8.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.11.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.11.norm1.weight | [768] |
| ernie.encoder.layer.9.attention.output.dense.weight | [768, 768] | ernie.encoder.layers.9.self_attn.out_proj.weight | [768, 768] |
| ernie.encoder.layer.10.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.10.norm1.bias | [768] |
| ernie.encoder.layer.0.attention.self.query.bias | [768] | ernie.encoder.layers.0.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.8.intermediate.dense.bias | [3072] | ernie.encoder.layers.8.linear1.bias | [3072] |
| ernie.encoder.layer.6.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.6.norm1.bias | [768] |
| ernie.encoder.layer.3.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.3.norm1.weight | [768] |
| ernie.encoder.layer.8.output.dense.weight | [768, 3072] | ernie.encoder.layers.8.linear2.weight | [3072, 768] |
| ernie.encoder.layer.8.attention.self.key.bias | [768] | ernie.encoder.layers.8.self_attn.k_proj.bias | [768] |
| cls.predictions.bias | [40000] | cls.predictions.decoder_bias | [40000] |
| ernie.encoder.layer.1.attention.self.value.bias | [768] | ernie.encoder.layers.1.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.7.output.LayerNorm.gamma | [768] | ernie.encoder.layers.7.norm2.weight | [768] |
| ernie.encoder.layer.1.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.1.norm1.bias | [768] |
| ernie.encoder.layer.7.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.7.linear1.weight | [768, 3072] |
| ernie.encoder.layer.9.output.dense.bias | [768] | ernie.encoder.layers.9.linear2.bias | [768] |
| ernie.encoder.layer.0.output.dense.bias | [768] | ernie.encoder.layers.0.linear2.bias | [768] |
| ernie.encoder.layer.10.attention.self.query.bias | [768] | ernie.encoder.layers.10.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.4.output.LayerNorm.beta | [768] | ernie.encoder.layers.4.norm2.bias | [768] |
| ernie.encoder.layer.6.output.dense.bias | [768] | ernie.encoder.layers.6.linear2.bias | [768] |
| ernie.encoder.layer.3.attention.output.dense.bias | [768] | ernie.encoder.layers.3.self_attn.out_proj.bias | [768] |
| ernie.encoder.layer.4.intermediate.dense.weight | [3072, 768] | ernie.encoder.layers.4.linear1.weight | [768, 3072] |
| ernie.encoder.layer.3.attention.self.query.weight | [768, 768] | ernie.encoder.layers.3.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.6.attention.self.query.weight | [768, 768] | ernie.encoder.layers.6.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.2.intermediate.dense.bias | [3072] | ernie.encoder.layers.2.linear1.bias | [3072] |
| ernie.encoder.layer.9.attention.output.LayerNorm.gamma | [768] | ernie.encoder.layers.9.norm1.weight | [768] |
| ernie.encoder.layer.9.output.LayerNorm.gamma | [768] | ernie.encoder.layers.9.norm2.weight | [768] |
| ernie.encoder.layer.10.output.dense.bias | [768] | ernie.encoder.layers.10.linear2.bias | [768] |
| ernie.encoder.layer.6.attention.self.query.bias | [768] | ernie.encoder.layers.6.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.2.output.dense.weight | [768, 3072] | ernie.encoder.layers.2.linear2.weight | [3072, 768] |
| ernie.encoder.layer.9.attention.self.query.weight | [768, 768] | ernie.encoder.layers.9.self_attn.q_proj.weight | [768, 768] |
| ernie.encoder.layer.9.attention.self.query.bias | [768] | ernie.encoder.layers.9.self_attn.q_proj.bias | [768] |
| ernie.encoder.layer.0.attention.self.value.bias | [768] | ernie.encoder.layers.0.self_attn.v_proj.bias | [768] |
| ernie.encoder.layer.11.output.dense.weight | [768, 3072] | ernie.encoder.layers.11.linear2.weight | [3072, 768] |
| ernie.encoder.layer.3.attention.output.LayerNorm.beta | [768] | ernie.encoder.layers.3.norm1.bias | [768] |
| ernie.encoder.layer.9.intermediate.dense.bias | [3072] | ernie.encoder.layers.9.linear1.bias | [3072] |
| ernie.encoder.layer.4.attention.self.query.bias | [768] | ernie.encoder.layers.4.self_attn.q_proj.bias | [768] |

```
# 上面结果形状不同的参数如下
# ernie.encoder.layer.0.intermediate.dense.weight	[3072, 768]	ernie.encoder.layers.0.linear1.weight	[768, 3072]
# ernie.encoder.layer.0.output.dense.weight	[768, 3072]	ernie.encoder.layers.0.linear2.weight	[3072, 768]

```

 PyTorch 对于 `nn.Linear` 模块的保存是将权重的 shape 进行转置后保存的



* 获取转换模型

```python
paddle_state_dict = {}
for torch_key in pytorch_state_dict.keys():
    paddle_key = weight_map[torch_key]

    if ('linear' in paddle_key) or ('proj' in  paddle_key) or ('vocab' in  paddle_key and 'weight' in  paddle_key) or ("dense.weight" in paddle_key) or ('transform.weight' in paddle_key) or ('seq_relationship.weight' in paddle_key):
        paddle_state_dict[paddle_key] = paddle.to_tensor(pytorch_state_dict[torch_key].cpu().numpy().transpose())

    else:
        paddle_state_dict[paddle_key] = paddle.to_tensor(pytorch_state_dict[torch_key].cpu().numpy())
        # print(torch_key)
        print("torch: ", torch_key,"\t", pytorch_state_dict[torch_key].shape)
        # print(paddle_key)
        print("paddle: ", paddle_key, "\t", paddle_state_dict[paddle_key].shape, "\n")

paddle.save(paddle_state_dict, paddle_model_path)
```



* 验证是否对齐操作

```python
def value_torch2paddle():
    ''''''
    torch_model_name = '/workspace/custom_project/pretrain_model_file/Ernie-Torch-model/ernie-3.0-base-zh'
    paddle_model_name = "./convert_torch_to_paddle/ernie_3.0_torch"

    from transformers import BertTokenizer
    from paddlenlp.transformers import ErnieTokenizer

    text = '恭喜完成torch到paddle的模型转换'
    torch_model = TorchErnieModel.from_pretrained(torch_model_name)
    torch_tokenizer = BertTokenizer.from_pretrained(torch_model_name)
    torch_model.eval()

    torch_inputs = torch_tokenizer(text, return_tensors="pt")
    print('torch input',torch_inputs)
    torch_outputs = torch_model(**torch_inputs)

    torch_logits = torch_outputs[0]
    torch_array = torch_logits.cpu().detach().numpy()
    print("torch_prediction_logits shape:{}".format(torch_array.shape))
    print("torch_prediction_logits:{}".format(torch_array))

    paddle_model = PaddleErnieModel.from_pretrained(paddle_model_name)
    paddle_tokenizer = ErnieTokenizer.from_pretrained(paddle_model_name)
    paddle_model.eval()

    paddle_inputs = paddle_tokenizer(text)
    print('paddle input',paddle_inputs)

    paddle_inputs = {k:paddle.to_tensor([v]) for (k, v) in paddle_inputs.items()}
    paddle_outputs = paddle_model(**paddle_inputs)

    paddle_logits = paddle_outputs[0]
    paddle_array = paddle_logits.numpy()
    print("paddle_prediction_logits shape:{}".format(paddle_array.shape))
    print("paddle_prediction_logits:{}".format(paddle_array))

    # the output logits should have the same shape
    assert torch_array.shape == paddle_array.shape, "the output logits should have the same shape, but got : {} and {} instead".format(torch_array.shape, paddle_array.shape)
    diff = torch_array - paddle_array
    print(np.amax(abs(diff)))

```












## paddle2torch

[解读网络结构转换](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/model_convert/convert_from_pytorch/convert_net_structure_cn.html)

```python
# -*- coding:utf-8 -*-
'''
# @FileName    :convert_paddle_to_torch.py
---------------------------------
# @Time        :2023/9/12 
# @Author      :zhangyj-n
# @Email       :13091375161@163.com
# @zhihu       :https://www.zhihu.com/people/zhangyj-n
# @kg-nlp      :https://kg-nlp.github.io/Algorithm-Project-Manual/
---------------------------------
# 目标任务 :
---------------------------------
'''
import collections
import os
import json
import paddle.fluid.dygraph as D
import torch
from paddle import fluid


def build_params_map(attention_num=12):
    """
    build params map from paddle-paddle's ERNIE to transformer's BERT
    :return:
    """
    weight_map = collections.OrderedDict({
        'ernie.embeddings.word_embeddings.weight': "ernie.embeddings.word_embeddings.weight",
        'ernie.embeddings.position_embeddings.weight': "ernie.embeddings.position_embeddings.weight",
        'ernie.embeddings.token_type_embeddings.weight': "ernie.embeddings.token_type_embeddings.weight",
        'ernie.embeddings.task_type_embeddings.weight': "ernie.embeddings.task_type_embeddings.weight",
        'ernie.embeddings.layer_norm.weight': 'ernie.embeddings.LayerNorm.gamma',
        'ernie.embeddings.layer_norm.bias': 'ernie.embeddings.LayerNorm.beta',
    })
    # add attention layers
    for i in range(attention_num):
        weight_map[f'ernie.encoder.layers.{i}.self_attn.q_proj.weight'] = f'ernie.encoder.layer.{i}.attention.self.query.weight'
        weight_map[f'ernie.encoder.layers.{i}.self_attn.q_proj.bias'] = f'ernie.encoder.layer.{i}.attention.self.query.bias'
        weight_map[f'ernie.encoder.layers.{i}.self_attn.k_proj.weight'] = f'ernie.encoder.layer.{i}.attention.self.key.weight'
        weight_map[f'ernie.encoder.layers.{i}.self_attn.k_proj.bias'] = f'ernie.encoder.layer.{i}.attention.self.key.bias'
        weight_map[f'ernie.encoder.layers.{i}.self_attn.v_proj.weight'] = f'ernie.encoder.layer.{i}.attention.self.value.weight'
        weight_map[f'ernie.encoder.layers.{i}.self_attn.v_proj.bias'] = f'ernie.encoder.layer.{i}.attention.self.value.bias'
        weight_map[f'ernie.encoder.layers.{i}.self_attn.out_proj.weight'] = f'ernie.encoder.layer.{i}.attention.output.dense.weight'
        weight_map[f'ernie.encoder.layers.{i}.self_attn.out_proj.bias'] = f'ernie.encoder.layer.{i}.attention.output.dense.bias'
        weight_map[f'ernie.encoder.layers.{i}.norm1.weight'] = f'ernie.encoder.layer.{i}.attention.output.LayerNorm.gamma'
        weight_map[f'ernie.encoder.layers.{i}.norm1.bias'] = f'ernie.encoder.layer.{i}.attention.output.LayerNorm.beta'
        weight_map[f'ernie.encoder.layers.{i}.linear1.weight'] = f'ernie.encoder.layer.{i}.intermediate.dense.weight'
        weight_map[f'ernie.encoder.layers.{i}.linear1.bias'] = f'ernie.encoder.layer.{i}.intermediate.dense.bias'
        weight_map[f'ernie.encoder.layers.{i}.linear2.weight'] = f'ernie.encoder.layer.{i}.output.dense.weight'
        weight_map[f'ernie.encoder.layers.{i}.linear2.bias'] = f'ernie.encoder.layer.{i}.output.dense.bias'
        weight_map[f'ernie.encoder.layers.{i}.norm2.weight'] = f'ernie.encoder.layer.{i}.output.LayerNorm.gamma'
        weight_map[f'ernie.encoder.layers.{i}.norm2.bias'] = f'ernie.encoder.layer.{i}.output.LayerNorm.beta'
    # add pooler
    weight_map.update(
        {
            'ernie.pooler.dense.weight': 'ernie.pooler.dense.weight',
            'ernie.pooler.dense.bias': 'ernie.pooler.dense.bias',
            'cls.predictions.transform.weight': 'cls.predictions.transform.dense.weight',   # 这几个在编码的时候不需要
            'cls.predictions.transform.bias': 'cls.predictions.transform.dense.bias',
            'cls.predictions.layer_norm.weight': 'cls.predictions.transform.LayerNorm.gamma',
            'cls.predictions.layer_norm.bias': 'cls.predictions.transform.LayerNorm.beta',
            'cls.predictions.decoder_bias': 'cls.predictions.bias',
            'cls.predictions.decoder_weight': 'cls.predictions.weight',
        }
    )
    return weight_map

def extract_and_convert(input_dir, output_dir):
    """
    抽取并转换
    :param input_dir:
    :param output_dir:
    :return:
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    print('=' * 20 + 'save config file' + '=' * 20)
    config = json.load(open(os.path.join(input_dir, 'config.json'), 'rt', encoding='utf-8'))
    if 'init_args' in config:
        config = config['init_args'][0]
    # del config['init_class']
    config['layer_norm_eps'] = 1e-12
    config['model_type'] = 'ernie'
    config['architectures'] = ["ErnieForMaskedLM"]  # or 'BertModel'
    config['intermediate_size'] = 4 * config['hidden_size']
    json.dump(config, open(os.path.join(output_dir, 'config.json'), 'wt', encoding='utf-8'), indent=4)
    print('=' * 20 + 'save vocab file' + '=' * 20)
    with open(os.path.join(input_dir, 'vocab.txt'), 'rt', encoding='utf-8') as f:
        words = f.read().splitlines()
    words = [word.split('\t')[0] for word in words]
    with open(os.path.join(output_dir, 'vocab.txt'), 'wt', encoding='utf-8') as f:
        for word in words:
            f.write(word + "\n")
    print('=' * 20 + 'extract weights' + '=' * 20)
    state_dict = collections.OrderedDict()
    weight_map = build_params_map(attention_num=config['num_hidden_layers'])

    with fluid.dygraph.guard():
        paddle_paddle_params, _ = D.load_dygraph(os.path.join(input_dir, 'model_state.pdparams'))
    for weight_name, weight_value in paddle_paddle_params.items():
        if 'weight' in weight_name:
            if 'ernie.encoder' in weight_name or 'ernie.pooler' in weight_name or 'cls.' in weight_name:
                weight_value = weight_value.transpose()
        #         return
        if weight_name not in weight_map:
            print('=' * 20, '[SKIP]', weight_name, '=' * 20)
            continue
        # state_dict[weight_map[weight_name]] = torch.FloatTensor(weight_value,dty)

        state_dict[weight_map[weight_name]] = torch.tensor(weight_value,dtype=torch.float32)
        print(weight_name, '->', weight_map[weight_name], weight_value.shape)
    torch.save(state_dict, os.path.join(output_dir, "pytorch_model.bin"))


if __name__ == '__main__':
    extract_and_convert('../../pretrain_model_file/ernie-3.0-medium-zh/model_600', './convert/ernie-3.0-medium-torch-600')
```





## paddle2onnx

[Paddle2ONNX](https://github.com/PaddlePaddle/Paddle2ONNX#%E5%8F%82%E6%95%B0%E9%80%89%E9%A1%B9)

```bash
paddle2onnx --model_dir /workspace/custom_project/model_transformer/convert_to_static/paddle_class_model \
            --model_filename model.pdmodel \
            --params_filename model.pdiparams \
            --save_file /workspace/custom_project/model_transformer/convert_to_onnx/paddle_class_model/model.onnx \
            --opset_version 13 \
            --enable_dev_version True \
            --enable_onnx_checker True

paddle2onnx --model_dir /workspace/custom_project/model_transformer/convert_to_static/pair_model_ms/1 \
            --model_filename model.pdmodel \
            --params_filename model.pdiparams \
            --save_file /workspace/custom_project/model_transformer/convert_to_onnx/pair_model_ms/1/model.onnx \
            --opset_version 13 \
            --enable_dev_version True \
            --enable_onnx_checker True

paddle2onnx --model_dir /workspace/custom_project/model_transformer/convert_to_static/seqcls_model_ms/1 \
            --model_filename model.pdmodel \
            --params_filename model.pdiparams \
            --save_file /workspace/custom_project/model_transformer/convert_to_onnx/seqcls_model_ms/1/model.onnx \
            --opset_version 13 \
            --enable_dev_version True \
            --enable_onnx_checker True
```



## torch2onnx

```python
# -*- coding:utf-8 -*-
'''
# @FileName    :convert_pytorch_to_onnx
---------------------------------
# @Time        :2023/1/29 
# @Author      :zhangyj
---------------------------------
# 目标任务 :
---------------------------------
'''

import torch
import torch.onnx
import onnxruntime
from torch.nn.functional import softmax
from transformers import AutoModelForMaskedLM, AutoTokenizer,BertForSequenceClassification
import time
from tqdm import tqdm

'''Pytorch自带转换函数 torch.onnx.export()  https://pytorch.org/docs/stable/onnx.html#torch-onnx'''
def torch2onnx():

    pretrained = './ALBERT-SOP-MLM-5/2'
    pretrained = './BERT-NSP-MLM-3/0'
    tokenizer = AutoTokenizer.from_pretrained(pretrained)
    model = AutoModelForMaskedLM.from_pretrained(pretrained)
    model.eval()
    inputtext = "[MASK]笼或容器在使用前应按允许承载载能力的两倍荷载进行试验"
    maskpos = tokenizer.encode(inputtext, add_special_tokens=True).index(103)
    inputtext_encode = tokenizer([inputtext],add_special_tokens=True, padding='max_length', max_length=64,return_token_type_ids=False,return_tensors='pt')
    # {'input_ids':tensor([[]]),'attention_mask':tensor([[]])}
    outputs = model(inputtext_encode['input_ids'],inputtext_encode['attention_mask'])
    prediction_scores = outputs.logits
    logit_prob = softmax(prediction_scores[0, maskpos], dim=-1).data.tolist()
    predicted_index = torch.argmax(prediction_scores[0, maskpos]).item()
    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
    print(predicted_token, logit_prob[predicted_index])
    # print(inputtext_encode)
    torch.onnx.export(model,
                      args =(inputtext_encode['input_ids'],
                       inputtext_encode['attention_mask']
                       ),
                      # f = './ALBERT-SOP-MLM-5/onnx-albert/albert.onnx',
                      f ='./BERT-NSP-MLM-3/onnx-bert/bert.onnx',
                      opset_version=10,
                      input_names = ['input_ids','attention_mask'],
                      output_names = ['outputs'],
                      dynamic_axes={'input_ids':{0:'batch_size',1:'max_length'},
                                    'attention_mask':{0:'batch_size',1:'max_length'}}
                      )


    pass

'''算子去重,精简模型'''
def onnx_process():
    from onnxruntime.transformers.onnx_model import OnnxModel
    import onnx

    def has_same_value(val_one, val_two):
        if val_one.raw_data == val_two.raw_data:
            return True
        else:
            return False

    path = f"./ALBERT-SOP-MLM-5/onnx-albert/albert.onnx"  # 242M
    output_path = f"./ALBERT-SOP-MLM-5/onnx-albert/albert_simple.onnx"  # 7.50M

    path = f"./BERT-NSP-MLM-3/onnx-bert/bert.onnx"  # 242M
    output_path = f"./BERT-NSP-MLM-3/onnx-bert/bert_simple.onnx"  # 7.50M

    model = onnx.load(path)
    onnx_model = OnnxModel(model)

    count = len(model.graph.initializer)
    same = [-1] * count
    for i in tqdm(range(count - 1)):
        if same[i] >= 0:
            continue
        for j in range(i + 1, count):
            if has_same_value(model.graph.initializer[i],
                              model.graph.initializer[j]):
                same[j] = i

    for i in tqdm(range(count)):
        if same[i] >= 0:
            onnx_model.replace_input_of_all_nodes(model.graph.initializer[i].name,
                                                  model.graph.initializer[same[i]].name)
    onnx_model.update_graph()
    onnx_model.save_model_to_file(output_path)


'''掩码任务测试'''
def pt_onnx_time():

    # pt模型加载
    pretrained = './ALBERT-SOP-MLM-5/2'
    pretrained = './BERT-NSP-MLM-3/0'
    tokenizer = AutoTokenizer.from_pretrained(pretrained)
    model = AutoModelForMaskedLM.from_pretrained(pretrained)
    inputtext = "[MASK]笼或容器在使用前应按允许承载载能力的两倍荷载进行试验"
    maskpos = tokenizer.encode(inputtext, add_special_tokens=True).index(103)
    start = time.time()
    inputtext_encode = tokenizer([inputtext]*10, add_special_tokens=True, padding='max_length', max_length=15, return_token_type_ids=False, return_tensors='pt')
    # {'input_ids':tensor([[]]),'attention_mask':tensor([[]])}
    outputs = model(inputtext_encode['input_ids'], inputtext_encode['attention_mask'])
    end = time.time()
    prediction_scores = outputs.logits
    # print(prediction_scores)
    logit_prob = softmax(prediction_scores[0, maskpos], dim=-1).data.tolist()
    predicted_index = torch.argmax(prediction_scores[0, maskpos]).item()
    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
    print('使用pt模型',end-start)
    print(predicted_token, logit_prob[predicted_index])
    # print(logit_prob[predicted_index])

    # onnx模型加载
    # sess = onnxruntime.InferenceSession('./ALBERT-SOP-MLM-5/onnx-albert/albert_simple.onnx', providers=['CPUExecutionProvider'])
    sess = onnxruntime.InferenceSession('./BERT-NSP-MLM-3/onnx-bert/bert.onnx', providers=['CPUExecutionProvider'])
    # sess = onnxruntime.InferenceSession('./BERT-NSP-MLM-3/onnx-bert/bert.onnx', providers=['CUDAExecutionProvider'])

    # sess = onnxruntime.InferenceSession('./ALBERT-SOP-MLM-5/onnx-albert/albert.onnx', providers=['CUDAExecutionProvider'])
    start = time.time()
    inputs_onnx = {k: v.cpu().detach().numpy() for k, v in inputtext_encode.items()}
    outputs = sess.run(None,inputs_onnx)
    end = time.time()
    prediction_scores = torch.tensor(outputs[0])
    logit_prob = softmax(prediction_scores[0, maskpos], dim=-1).data.tolist()
    predicted_index = torch.argmax(prediction_scores[0, maskpos]).item()
    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
    print('使用onnx模型',end-start)
    print(predicted_token, logit_prob[predicted_index])

if __name__ == '__main__':

    # torch2onnx()
    # onnx_process()
    pt_onnx_time()

    # pretrained = './ALBERT-SOP-MLM-5/2'
    # tokenizer = AutoTokenizer.from_pretrained(pretrained)
    # inputtext = "[MASK]笼或容器在使用前应按允许承载载能力的两倍荷载进行试验"
    # # res = tokenizer.encode_plus(inputtext, add_special_tokens=True, padding='max_length', max_length=64,return_token_type_ids=False)
    # res = tokenizer([inputtext],add_special_tokens=True, padding='max_length', max_length=64,return_token_type_ids=False,return_tensors='pt')
    # print(res)
    pass
```





## torch2trition



## paddle动态图/静态图互转

* 动态图文件说明

```
pdmodel是训练使用的完整模型 program 描述，区别于推理模型，训练模型 program 包含完整的网络，包括前向网络，反向网络和优化器
pdparams是训练网络的参数 dict，key 为变量名，value 为 Tensor array 数值
pdopt是训练优化器的参数，结构与*.pdparams 一致
```

* 静态图文件说明

```
pdmodel为推理使用的模型 program 描述
pdiparams为推理用的参数,存储格式与*.pdparams不同,pdiparams为二进制 Tensor 存储格式，不含变量名
```

* 动转静

```python
# -*- coding:utf-8 -*-
'''
# @FileName    :paddle_dygraph_to_static.py
---------------------------------
# @Time        :2023/9/21 
# @Author      :zhangyj-n
# @Email       :13091375161@163.com
# @zhihu       :https://www.zhihu.com/people/zhangyj-n
# @kg-nlp      :https://kg-nlp.github.io/Algorithm-Project-Manual/
---------------------------------
# 目标任务 :   paddle模型动态图转静态图
---------------------------------
'''
import sys
import os
from loguru import logger
from tqdm import tqdm

''''''
import argparse
import os

import paddle
from paddlenlp.transformers import AutoModelForSequenceClassification

# yapf: disable
parser = argparse.ArgumentParser()
parser.add_argument('--multilingual', action='store_true', help='Whether is multilingual task')
parser.add_argument("--params_path", type=str, default='./test_model/paddle_class_model')
parser.add_argument("--output_path", type=str, default='./convert_to_static/paddle_class_model')


args = parser.parse_args()
# yapf: enable

if __name__ == "__main__":

    model = AutoModelForSequenceClassification.from_pretrained(args.params_path)
    model.eval()
    # if args.multilingual:
    #     input_spec = [paddle.static.InputSpec(shape=[None, None], dtype="int64", name="input_ids")]
    # else:
    input_spec = [
        paddle.static.InputSpec(shape=[None, None], dtype="int64", name="input_ids"),
        paddle.static.InputSpec(shape=[None, None], dtype="int64", name="token_type_ids"),
    ]
    # Convert to static graph with specific input description
    model = paddle.jit.to_static(model, input_spec=input_spec)

    # Save in static graph model.
    save_path = os.path.join(args.output_path, "float32")
    paddle.jit.save(model, save_path)


if __name__ == '__main__':

    pass
```

* 静转动

```python
import argparse
import paddle
from paddlenlp.transformers import AutoModelForPretraining
from paddlenlp.utils.log import logger

paddle.set_device("cpu")
parser = argparse.ArgumentParser()
parser.add_argument("--model", type=str, help="The name of pretrained weights in PaddleNLP.")
parser.add_argument("--path", type=str, help="The path of checkpoint to be loaded.")
parser.add_argument("--output_path", type=str, default=None, help="The path of checkpoint to be loaded.")
args = parser.parse_args()


def init_dygraph_with_static(model, static_params_path):
    from paddlenlp.utils.tools import static_params_to_dygraph

    static_tensor_dict = paddle.static.load_program_state(static_params_path)
    return static_params_to_dygraph(model, static_tensor_dict)


def main(args):
    logger.info("Loading model: %s" % args.model)
    model = AutoModelForPretraining.from_pretrained(args.model)
    logger.info("Loading static params and trans paramters...")
    model_dict = init_dygraph_with_static(model, args.path)
    save_name = args.output_path
    if save_name is None:
        save_name = args.model + "_converted.pdparams"
    if not save_name.endswith(".pdparams"):
        save_name += ".pdparams"
    logger.info("Saving converted params to %s" % save_name)
    paddle.save(model_dict, save_name)
    logger.info("New pdparams saved!")


if __name__ == "__main__":
    main(args)

```





## onnx2trition

* 最好在triton server环境下操作

```python
/usr/src/tensorrt/bin/trtexec --onnx=/model_inference/onnx_backend/models/pair_model_ms/1/model.onnx \
    --saveEngine=model_inference/tensort_backend/models/pair_model_ms/1/model.plan \
    --explicitBatch=implicit

/usr/src/tensorrt/bin/trtexec --onnx=/model_inference/onnx_backend/models/seqcls_model_ms/1/model.onnx \
    --saveEngine=model_inference/tensort_backend/models/seqcls_model_ms/1/model.plan \
    --explicitBatch
    
/usr/src/tensorrt/bin/trtexec --onnx=/model_inference/onnx_backend/models/seqcls_model_sps/1/model.onnx \
    --saveEngine=model_inference/tensort_backend/models/seqcls_model_sps/1/model.plan \
    --explicitBatch
    
    
import tensorrt as trt

def onnx2trt(model_version_dir, onnx_model_file, max_batch):
    logger = trt.Logger(trt.Logger.WARNING)

    builder = trt.Builder(logger)

    # The EXPLICIT_BATCH flag is required in order to import models using the ONNX parser
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))

    parser = trt.OnnxParser(network, logger)

    success = parser.parse_from_file(onnx_model_file)
    for idx in range(parser.num_errors):
        print(parser.get_error(idx))

    print(success)  # True
    if not success:
        pass  # Error handling code here

    profile = builder.create_optimization_profile()
    # INPUT0可以接收[1, 2] -> [max_batch, 2]的维度
    profile.set_shape("input_ids", [1, 128], [1, 128], [max_batch, 128])
    profile.set_shape("token_type_ids", [1, 128], [1, 128], [max_batch, 128])

    config = builder.create_builder_config()
    config.add_optimization_profile(profile)

    # tensorrt 8.x
    # config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 20)  # 1 MiB

    # tensorrt 7.x
    # config.max_workspace_size = 1 << 20

    try:
        engine_bytes = builder.build_serialized_network(network, config)
    except AttributeError:
        engine = builder.build_engine(network, config)
        engine_bytes = engine.serialize()
        del engine
    print(engine_bytes)
    with open(os.path.join(model_version_dir, 'model.plan'), "wb") as f:
        f.write(engine_bytes)

if __name__ == '__main__':
    onnx2trt('/model_inference/tensort_backend/models/pair_model_ms/1',
             '/model_inference/onnx_backend/models/pair_model_ms/1/model.onnx',
             256)

    onnx2trt('/model_inference/tensort_backend/models/seqcls_model_ms/1',
             '/model_inference/onnx_backend/models/seqcls_model_ms/1/model.onnx',
             256)

    onnx2trt('/model_inference/tensort_backend/models/seqcls_model_sps/1',
             '/model_inference/onnx_backend/models/seqcls_model_sps/1/model.onnx',
             256)

    pass

```



## paddle2triton





## torch2tf

```python
# coding=utf-8
# Copyright 2018 The HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""" Convert pytorch checkpoints to TensorFlow"""


import sys
sys.path.append('./')

import argparse
import os


from transformers import *
from transformers.utils import CONFIG_NAME, WEIGHTS_NAME, cached_file, logging


if is_torch_available():
    import numpy as np
    import torch

    from transformers import*

logging.set_verbosity_info()

MODEL_CLASSES = {
    "bert": (
        BertConfig,
        TFBertForPreTraining,
        BertForPreTraining,
        BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,
    )
}


def convert_pt_checkpoint_to_tf(
    model_type, pytorch_checkpoint_path, config_file, tf_dump_path, compare_with_pt_model=False, use_cached_models=True
):
    if model_type not in MODEL_CLASSES:
        raise ValueError(f"Unrecognized model type, should be one of {list(MODEL_CLASSES.keys())}.")

    config_class, model_class, pt_model_class, aws_config_map = MODEL_CLASSES[model_type]
    # Initialise TF model
    if config_file in aws_config_map:
        config_file = cached_file(config_file, CONFIG_NAME, force_download=not use_cached_models)
    # print(config_class,config_file)  # <class 'transformers.models.albert.configuration_albert.AlbertConfig'> ./ALBERT-SOP-MLM-5/2/
    config = config_class.from_json_file(config_file)
    config.output_hidden_states = True
    config.output_attentions = True
    print(f"Building TensorFlow model from configuration: {config}")
    tf_model = model_class(config)

    # Load weights from tf checkpoint
    if pytorch_checkpoint_path in aws_config_map.keys():
        pytorch_checkpoint_path = cached_file(
            pytorch_checkpoint_path, WEIGHTS_NAME, force_download=not use_cached_models
        )
    # Load PyTorch checkpoint in tf2 model:
    tf_model = load_pytorch_checkpoint_in_tf2_model(tf_model, pytorch_checkpoint_path)

    if compare_with_pt_model:
        tfo = tf_model(tf_model.dummy_inputs, training=False)  # build the network

        state_dict = torch.load(pytorch_checkpoint_path, map_location="cpu")
        pt_model = pt_model_class.from_pretrained(
            pretrained_model_name_or_path=None, config=config, state_dict=state_dict
        )

        with torch.no_grad():
            pto = pt_model(**pt_model.dummy_inputs)

        np_pt = pto[0].numpy()
        np_tf = tfo[0].numpy()
        diff = np.amax(np.abs(np_pt - np_tf))
        print(f"Max absolute difference between models outputs {diff}")
        assert diff <= 2e-2, f"Error, model absolute difference is >2e-2: {diff}"

    # Save pytorch-model
    print(f"Save TensorFlow model to {tf_dump_path}")
    tf_model.save_weights(tf_dump_path, save_format="h5")


def convert_all_pt_checkpoints_to_tf(
    args_model_type,
    tf_dump_path,
    model_shortcut_names_or_path=None,
    config_shortcut_names_or_path=None,
    compare_with_pt_model=False,
    use_cached_models=False,
    remove_cached_files=False,
    only_convert_finetuned_models=False,
):

    if args_model_type is None:
        model_types = list(MODEL_CLASSES.keys())
    else:
        model_types = [args_model_type]

    for j, model_type in enumerate(model_types, start=1):
        print("=" * 100)
        print(f" Converting model type {j}/{len(model_types)}: {model_type}")
        print("=" * 100)
        if model_type not in MODEL_CLASSES:
            raise ValueError(f"Unrecognized model type {model_type}, should be one of {list(MODEL_CLASSES.keys())}.")

        print(MODEL_CLASSES[model_type])
        config_class, model_class, pt_model_class, aws_model_maps, aws_config_map = MODEL_CLASSES[model_type]
        # config_class, model_class, pt_model_class, aws_config_map = MODEL_CLASSES[model_type]


        if model_shortcut_names_or_path is None:
            model_shortcut_names_or_path = list(aws_model_maps.keys())
        if config_shortcut_names_or_path is None:
            config_shortcut_names_or_path = model_shortcut_names_or_path

        for i, (model_shortcut_name, config_shortcut_name) in enumerate(
            zip(model_shortcut_names_or_path, config_shortcut_names_or_path), start=1
        ):
            print("-" * 100)
            if "-squad" in model_shortcut_name or "-mrpc" in model_shortcut_name or "-mnli" in model_shortcut_name:
                if not only_convert_finetuned_models:
                    print(f"    Skipping finetuned checkpoint {model_shortcut_name}")
                    continue
                model_type = model_shortcut_name
            elif only_convert_finetuned_models:
                print(f"    Skipping not finetuned checkpoint {model_shortcut_name}")
                continue
            print(
                f"    Converting checkpoint {i}/{len(aws_config_map)}: {model_shortcut_name} - model_type {model_type}"
            )
            print("-" * 100)

            if config_shortcut_name in aws_config_map:
                config_file = cached_file(config_shortcut_name, CONFIG_NAME, force_download=not use_cached_models)
            else:
                config_file = config_shortcut_name

            if model_shortcut_name in aws_model_maps:
                model_file = cached_file(model_shortcut_name, WEIGHTS_NAME, force_download=not use_cached_models)
            else:
                model_file = model_shortcut_name

            if os.path.isfile(model_shortcut_name):
                model_shortcut_name = "converted_model"

            convert_pt_checkpoint_to_tf(
                model_type=model_type,
                pytorch_checkpoint_path=model_file,
                config_file=config_file,
                tf_dump_path=os.path.join(tf_dump_path, model_shortcut_name + "-tf_model.h5"),
                compare_with_pt_model=compare_with_pt_model,
            )
            if remove_cached_files:
                os.remove(config_file)
                os.remove(model_file)



def main():
    parser = argparse.ArgumentParser()
    # Required parameters
    parser.add_argument(
        "--tf_dump_path", default='./ALBERT-SOP-MLM-5/tf-albert/tf_model.h5', type=str, required=False, help="Path to the output Tensorflow dump file."
    )
    parser.add_argument(
        "--model_type",
        default='albert',
        type=str,
        help=(
            f"Model type selected in the list of {list(MODEL_CLASSES.keys())}. If not given, will download and "
            "convert all the models from AWS."
        ),
    )
    parser.add_argument(
        "--pytorch_checkpoint_path",
        default='./ALBERT-SOP-MLM-5/2/pytorch_model.bin',
        type=str,
        help=(
            "Path to the PyTorch checkpoint path or shortcut name to download from AWS. "
            "If not given, will download and convert all the checkpoints from AWS."
        ),
    )
    parser.add_argument(
        "--config_file",
        default='./ALBERT-SOP-MLM-5/2/config.json',
        type=str,
        help=(
            "The config json file corresponding to the pre-trained model. \n"
            "This specifies the model architecture. If not given and "
            "--pytorch_checkpoint_path is not given or is a shortcut name "
            "use the configuration associated to the shortcut name on the AWS"
        ),
    )
    parser.add_argument(
        "--compare_with_pt_model", action="store_true", help="Compare Tensorflow and PyTorch model predictions."
    )
    parser.add_argument(
        "--use_cached_models",
        action="store_true",
        help="Use cached models if possible instead of updating to latest checkpoint versions.",
    )
    parser.add_argument(
        "--remove_cached_files",
        action="store_true",
        help="Remove pytorch models after conversion (save memory when converting in batches).",
    )
    parser.add_argument("--only_convert_finetuned_models", default=False, type=bool, help="Only convert finetuned models.")
    args = parser.parse_args()

    if args.pytorch_checkpoint_path is not None:
        convert_pt_checkpoint_to_tf(args.model_type.lower(),
                                    args.pytorch_checkpoint_path,
                                    args.config_file if args.config_file is not None else args.pytorch_checkpoint_path,
                                    args.tf_dump_path,
                                    compare_with_pt_model=args.compare_with_pt_model,
                                    use_cached_models=args.use_cached_models)
    # else:
    # convert_all_pt_checkpoints_to_tf(
    #     args.model_type.lower() if args.model_type is not None else None,
    #     args.tf_dump_path,
    #     model_shortcut_names_or_path=[args.pytorch_checkpoint_path]
    #     if args.pytorch_checkpoint_path is not None
    #     else None,
    #     config_shortcut_names_or_path=[args.config_file] if args.config_file is not None else None,
    #     compare_with_pt_model=args.compare_with_pt_model,
    #     use_cached_models=args.use_cached_models,
    #     remove_cached_files=args.remove_cached_files,
    #     only_convert_finetuned_models=args.only_convert_finetuned_models,
    # )

'''加载tf模型进行测试'''
def load_tf_model():
    # albert 无法实现

    tf_cache_dir = './ALBERT-SOP-MLM-5/tf-albert'


    from transformers import BertTokenizer, AlbertForMaskedLM, BertForMaskedLM, AutoModelForMaskedLM, RobertaForMaskedLM, AutoTokenizer,TFBertForMaskedLM,AutoModel
    import torch
    from torch.nn.functional import softmax

    model = AutoModelForMaskedLM.from_pretrained(tf_cache_dir, from_tf=True)

    print(model)
    # tokenizer = AutoTokenizer.from_pretrained(tf_cache_dir)
    # inputtext = "今天[MASK]情很好"
    # maskpos = tokenizer.encode(inputtext, add_special_tokens=True).index(103)
    # print(maskpos)
    # input_ids = torch.tensor(tokenizer.encode(inputtext, add_special_tokens=True)).unsqueeze(0)  # Batch size 1
    # outputs = model(input_ids, labels=input_ids)
    # loss, prediction_scores = outputs[:2]
    # logit_prob = softmax(prediction_scores[0, maskpos], dim=-1).data.tolist()
    # predicted_index = torch.argmax(prediction_scores[0, maskpos]).item()
    # predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
    # print(predicted_token, logit_prob[predicted_index])


if __name__ == "__main__":


    # main()  # pytorch 转tf
    load_tf_model()

```





## PyTorch 1.13 与 Paddle 2.4 API 映射表

[链接](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/model_convert/convert_from_pytorch/pytorch_api_mapping_cn.html#pytorch-1-13-paddle-2-4-api)

>  目前使用的开发环境 registry.cn-beijing.aliyuncs.com/zhangyj-n/paddle_torch:base-1.13.0-2.4.2-11.7



## 关键参考

````
```
@misc{nghuyong2019@ERNIE-Pytorch,
  title={ERNIEPytorch},
  author={Yong Hu},
  howpublished={\url{https://github.com/nghuyong/ERNIE-Pytorch}},
  year={2019}
}
```
````

