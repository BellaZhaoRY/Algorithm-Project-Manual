---
sort: 2
---


# 向量搜索工程

* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/向量表示/向量搜索工程.html)

* [个人知乎](https://www.zhihu.com/people/zhangyj-n)

* [vector_search](https://github.com/kg-nlp/vector_search)


> 向量工具:概念,特点,性能
> 工具部署,向量库插入,向量库更新,向量库搜索

目前 vector_search 支持如下模型,

```python
model_dim = {
    'm3e-small':512,
    'm3e-base':768,
    'm3e-large':1024 ,
    'm3e-base-custom': 768,
    'bge-small-zh':512 ,
    'bge-base-zh':768 ,
    'bge-large-zh':1024,
    'text2vec-base-chinese':768,
    'text2vec-large-chinese':1024,
    'text2vec-base-chinese-sentence':768,
    'text2vec-base-chinese-paraphrase':768,
    'text2vec-bge-large-chinese':1024,
    'Ernie-model/ernie-1.0-base-zh':768,
    'Ernie-model/ernie-1.0-large-zh-cw':1024,
    "Ernie-model/ernie-3.0-medium-zh":768,
    'Ernie-model/ernie-3.0-base-zh':768,
    'Ernie-model/rocketqa-zh-dureader-query-encoder':768,
    'Ernie-model/rocketqa-zh-base-query-encoder': 768,
    "Ernie-model/rocketqa-zh-dureader-query-encoder-256":256,
}
```

* 支持加载torch,paddle预训练模型,

* 支持paddle自定义模型,

* 支持三种向量库插入,更新,召回



## Miluvs

```python
# -*- coding:utf-8 -*-
'''
# @FileName    :milvus_tool.py
---------------------------------
# @Time        :2023/9/8 
# @Author      :zhangyj-n
# @Email       :13091375161@163.com
# @zhihu       :https://www.zhihu.com/people/zhangyj-n
# @kg-nlp      :https://kg-nlp.github.io/Algorithm-Project-Manual/
---------------------------------
# 目标任务 :  使用milvus构建向量库
---------------------------------
'''
from conf.config import MILVUS_HOST,MILVUS_PORT
from milvus import Milvus
import numpy as np
import pandas as pd
from loguru import logger
import json
from tqdm import tqdm


'''milvus配置'''
class VecToMilvus:
    def __init__(self):
        self.client = Milvus(host=MILVUS_HOST, port=MILVUS_PORT)

    def has_collection(self, collection_name):
        try:
            status, ok = self.client.has_collection(collection_name)
            return ok
        except Exception as e:
            logger.info("Milvus 读取 {} 错误:".format(collection_name)+str(e))

    def creat_collection(self, collection_name,collection_param):
        try:
            status = self.client.create_collection(collection_param)
            return status
        except Exception as e:
            logger.info("Milvus 创建集合 {} 错误:".format(collection_name)+str(e))

    def create_index(self, collection_name,index_type, index_param):
        try:
            status = self.client.create_index(collection_name, index_type, index_param)
            return status
        except Exception as e:
            logger.info("Milvus 创建索引 {} 错误:".format(collection_name)+str(e))

    def has_partition(self, collection_name, partition_tag):
        try:
            status, ok = self.client.has_partition(collection_name, partition_tag)
            return ok
        except Exception as e:
            logger.info("Milvus 分区 {} 错误: ".format(partition_tag)+str(e))

    def delete_partition(self, collection_name, partition_tag):
        try:
            status = self.client.drop_partition(collection_name,partition_tag)
            return status
        except Exception as e:
            logger.info("Milvus 删除分区 {} error: ".format(partition_tag)+str(e))

    def create_partition(self, collection_name, partition_tag):
        try:
            status = self.client.create_partition(collection_name, partition_tag)
            return status
        except Exception as e:
            logger.info("Milvus 创建分区 {} error: ".format(partition_tag)+str(e))

    def insert(self, vectors, index_type, index_param,ids=None, partition_tag=None,**collection_param):

        collection_name = collection_param['collection_name']
        try:
            if not self.has_collection(collection_name):
                self.creat_collection(collection_name,collection_param)
                self.create_index(collection_name,index_type, index_param)
                logger.info("集合信息: {}".format(self.client.get_collection_info(collection_name)[1]))
            if (partition_tag is not None) and (not self.has_partition(collection_name, partition_tag)):
                self.create_partition(collection_name, partition_tag)
            status, ids = self.client.insert(
                collection_name=collection_name, records=vectors, ids=ids, partition_tag=partition_tag
            )
            self.client.flush([collection_name])
            logger.info(
                "插入 {} 数据, 还有 {} 数据.".format(
                    len(ids), self.client.count_entities(collection_name)[1]
                )
            )
            return status, ids
        except Exception as e:
            logger.info("Milvus 插入错误:"+str(e))

'''召回'''
class RecallByMilvus:
    def __init__(self):
        self.client = Milvus(host=MILVUS_HOST, port=MILVUS_PORT)

    def search(self, collection_name,query_records, recall_num,search_param, partition_tag=None):
        try:
            status, results = self.client.search(
                collection_name=collection_name,
                query_records=query_records,
                top_k=recall_num,
                params=search_param,
                partition_tag=partition_tag,
            )
            return status, results
        except Exception as e:
            logger.info("Milvus 召回错误: "+ str(e))


class Milvus_recall():
    def __init__(self,**kwargs):
        # Initializing index
        self.collection_name = kwargs['collection_name']
        self.partition_tag = kwargs['partition_tag']
        self.index_type = kwargs['index_type']
        self.index_param = {"nlist":kwargs['nlist']}  # 每个文件中的向量类的个数，默认值为 16384
        self.search_param = {"nprobe":kwargs['nprobe']}  # 查询所涉及的向量类的个数。nprobe 影响查询精度。数值越大，精度越高，但查询速度更慢。[1,nlist]
        self.dimension = kwargs['dimension']
        self.metric_type = kwargs['metric_type']
        self.index_file_size = kwargs['index_file_size']  # 触发创建索引的阈值。该参数指定只有当原始数据文件大小达到某一阈值，系统才会为其创建索引，默认值为1024 MB。小于该阈值的数据文件不会创建索引。
        logger.info("初始化index完成")

    '''向量id映射'''
    def gen_id2corpus(self,corpus_file):
        self.id2corpus = {}
        with open(corpus_file, "r", encoding="utf-8") as f:
            for idx, line in enumerate(f.readlines()[1:]):
                self.id2corpus[idx] = line.rstrip()
        logger.info("召回库总数量:{}".format(len(self.id2corpus)))
        # print(json.dumps(self.id2corpus,indent=2,ensure_ascii=False))
        return self.id2corpus

    '''向量存储'''
    def milvus_index(self,npy_file,cover=False):
        logger.info('loading... '+npy_file)
        all_embeddings = np.load(npy_file)
        data_size = all_embeddings.shape[0]
        embedding_ids = [i for i in range(data_size)]
        batch_size = 10000

        client = VecToMilvus()
        if client.has_partition(self.collection_name, self.partition_tag):
            if cover:
                client.delete_partition(self.collection_name, self.partition_tag)
            else:
                logger.info('已经存在集合分区 {} {} '.format(self.collection_name,self.partition_tag))
                return

        for i in tqdm(range(0, data_size, batch_size)):
            cur_end = i + batch_size
            if cur_end > data_size:
                cur_end = data_size
            batch_emb = all_embeddings[np.arange(i, cur_end)]
            client.insert(
                vectors=batch_emb.tolist(),
                index_type=self.index_type,
                index_param=self.index_param,
                ids=embedding_ids[i: i + batch_size],
                partition_tag=self.partition_tag,
                **{"dimension":self.dimension,"index_file_size":self.index_file_size,"metric_type":self.metric_type,"collection_name":self.collection_name}
            )
        logger.info("index类说明 "+str(client))

    '''向量召回,输入query_embedding,输出结果文件'''
    def milvus_recall_file(self,query_total_embedding:list,query_list:list,pre_batch_size,recall_result_file:str,recall_num):

        client = RecallByMilvus()
        total_list = []
        with open(recall_result_file, "w", encoding="utf-8") as f:
            for batch_index, batch_query_embedding in enumerate(query_total_embedding):
                status, results = client.search(
                    collection_name=self.collection_name, query_records=batch_query_embedding,
                    recall_num=recall_num,search_param=self.search_param,partition_tag=self.partition_tag,
                )
                batch_size = len(results)
                for row_index in range(batch_size):
                    text_index = pre_batch_size * batch_index + row_index
                    for item in results[row_index]:
                        idx = item.id
                        f.write(
                            "{}\t{}\t{}\n".format(
                                query_list[text_index], self.id2corpus[idx], item.distance
                            )
                        )

if __name__ == '__main__':
    pass
```



## Faiss

```python
# -*- coding:utf-8 -*-
'''
# @FileName    :faiss_tool.py
---------------------------------
# @Time        :2023/9/8 
# @Author      :zhangyj-n
# @Email       :13091375161@163.com
# @zhihu       :https://www.zhihu.com/people/zhangyj-n
# @kg-nlp      :https://kg-nlp.github.io/Algorithm-Project-Manual/
---------------------------------
# 目标任务 :  使用faiss构建向量库
---------------------------------
'''

import faiss
import numpy as np
import pandas as pd
from loguru import logger
import json
from tqdm import tqdm

class Faiss_recall():
    def __init__(self,**kwargs):
        # Initializing index

        dim = kwargs['output_embedding_size']
        self.space = kwargs['distance']  # metric
        nlist  = kwargs['nlist']  # 聚类中心个数
        if self.space == 'ip':
            self.index = faiss.IndexFlatIP(dim)
        elif self.space == 'l2':
            self.index = faiss.IndexFlatL2(dim)

        logger.info("初始化index完成")

    '''向量id映射'''
    def gen_id2corpus(self,corpus_file):
        self.id2corpus = {}
        with open(corpus_file, "r", encoding="utf-8") as f:
            for idx, line in enumerate(f.readlines()[1:]):
                self.id2corpus[idx] = line.rstrip()
        logger.info("召回库总数量:{}".format(len(self.id2corpus)))
        # print(json.dumps(self.id2corpus,indent=2,ensure_ascii=False))
        return self.id2corpus

    '''向量存储'''
    def faiss_index(self,npy_file):
        logger.info('loading... '+npy_file)
        all_embeddings = np.load(npy_file)
        data_size = all_embeddings.shape[0]
        batch_size = 10000
        for i in tqdm(range(0, data_size, batch_size)):
            cur_end = i + batch_size
            if cur_end > data_size:
                cur_end = data_size
            batch_emb = all_embeddings[np.arange(i, cur_end)]
            self.index.add(batch_emb.astype('float32'))
            # faiss.write_index(indexer, collection_name)
        logger.info("index类说明 "+str(self.index))
        return self.index

    '''向量召回,输入query_embedding,输出结果文件'''
    def faiss_recall_file(self,query_total_embedding:list,query_list:list,pre_batch_size,recall_result_file:str,recall_num):

        total_list = []
        with open(recall_result_file, "w", encoding="utf-8") as f:
            if self.space == 'ip':
                for batch_index, batch_query_embedding in enumerate(query_total_embedding):
                    cosine_sims,recalled_idx = self.index.search(batch_query_embedding, recall_num)
                    batch_size = len(cosine_sims)
                    for row_index in range(batch_size):
                        text_index = pre_batch_size * batch_index + row_index
                        for idx, doc_idx in enumerate(recalled_idx[row_index]):
                            # total_list.append([query_list[text_index], id2corpus[doc_idx], 1.0 - cosine_sims[row_index][idx]])
                            f.write(
                                "{}\t{}\t{}\n".format(
                                    query_list[text_index], self.id2corpus[doc_idx], 1.0-cosine_sims[row_index][idx]
                                )
                            )
            elif self.space == 'l2':
                for batch_index, batch_query_embedding in enumerate(query_total_embedding):
                    l2_sims,recalled_idx  = self.index.search(batch_query_embedding, recall_num)
                    batch_size = len(l2_sims)
                    for row_index in range(batch_size):
                        text_index = pre_batch_size * batch_index + row_index
                        for idx, doc_idx in enumerate(recalled_idx[row_index]):
                            f.write(
                                "{}\t{}\t{}\n".format(
                                    query_list[text_index], self.id2corpus[doc_idx], l2_sims[row_index][idx]
                                )
                            )

    '''向量召回,输入query_embedding,输出单个或几个召回结果'''
    def hnswlib_search(self,batch_query_embedding:list,query_list:list,recall_num):
        total_list = []
        if self.space == 'ip':
            cosine_sims,recalled_idx = self.index.search(batch_query_embedding, recall_num)
            batch_size = len(cosine_sims)
            for row_index in range(batch_size):
                for idx, doc_idx in enumerate(recalled_idx[row_index]):
                    total_list.append([query_list[row_index], self.id2corpus[doc_idx], 1-cosine_sims[row_index][idx]])
        elif self.space == 'l2':
            for batch_index, batch_query_embedding in enumerate(query_total_embedding):
                l2_sims,recalled_idx = self.index.search(batch_query_embedding, recall_num)
                batch_size = len(l2_sims)
                for row_index in range(batch_size):
                    for idx, doc_idx in enumerate(recalled_idx[row_index]):
                        total_list.append([query_list[row_index], self.id2corpus[doc_idx], l2_sims[row_index][idx]])
        print(total_list)


if __name__ == '__main__':
    pass


```





## [hnswlib](https://github.com/nmslib/hnswlib)

```python
# -*- coding:utf-8 -*-
'''
# @FileName    :hnswlib_tool.py
---------------------------------
# @Time        :2023/9/6 
# @Author      :zhangyj-n
# @Email       :zhangyj-n@glodon.com
---------------------------------
# 目标任务 :   使用hnswlib构建向量库
---------------------------------
'''
import hnswlib
import numpy as np
import pandas as pd
from loguru import logger
import json
class Hnswlib_recall():
    def __init__(self,**kwargs):
        # Initializing index
        # max_elements - the maximum number of elements (capacity). Will throw an exception if exceeded
        # during insertion of an element.
        # The capacity can be increased by saving/loading the index, see below.

        # ef_construction - controls index search speed/build speed tradeoff

        # M - is tightly connected with internal dimensionality of the data. Strongly affects memory consumption (~M)
        # Higher M leads to higher accuracy/run_time at fixed ef/efConstruction

        # Controlling the recall by setting ef:
        # higher ef leads to better accuracy, but slower search

        # Set number of threads used during batch search/construction
        # By default using all available cores

        dim = kwargs['output_embedding_size']  #
        self.space = kwargs['distance']  # metric
        max_elements = kwargs['hnsw_max_elements']  # the maximum number of elements that can be stored in the structure(can be increased/shrunk)
        ef_construction = kwargs['hnsw_ef']  # a construction time/accuracy trade-off
        M = kwargs['hnsw_m']  # ha maximum number of outgoing connections in the graph
        num_threads = kwargs['hnsw_threads']  # he number of cpu threads to use
        self.index = hnswlib.Index(space=self.space, dim=dim)
        self.index.init_index(max_elements=max_elements, ef_construction=ef_construction, M=M)
        self.index.set_ef(ef_construction)
        self.index.set_num_threads(num_threads)
        logger.info("初始化index完成")

    '''向量id映射'''
    def gen_id2corpus(self,corpus_file):
        self.id2corpus = {}
        with open(corpus_file, "r", encoding="utf-8") as f:
            for idx, line in enumerate(f.readlines()[1:]):
                self.id2corpus[idx] = line.rstrip()
        logger.info("召回库总数量:{}".format(len(self.id2corpus)))
        # print(json.dumps(self.id2corpus,indent=2,ensure_ascii=False))
        return self.id2corpus

    '''向量存储'''
    def hnswlib_index(self,npy_file):
        logger.info('loading... '+npy_file)
        all_embeddings = np.load(npy_file)
        self.index.add_items(all_embeddings)
        logger.info("总索引数:{}".format(self.index.get_current_count()))
        logger.info("index类说明 "+str(self.index))
        # print(self.index.get_ids_list())
        return self.index

    '''向量召回,输入query_embedding,输出结果文件'''
    def hnswlib_recall_file(self,query_total_embedding:list,query_list:list,pre_batch_size,recall_result_file:str,recall_num):

        total_list = []
        with open(recall_result_file, "w", encoding="utf-8") as f:
            if self.space == 'ip':
                for batch_index, batch_query_embedding in enumerate(query_total_embedding):
                    recalled_idx, cosine_sims = self.index.knn_query(batch_query_embedding, recall_num)
                    # print(recalled_idx,cosine_sims)
                    # print(recalled_idx)
                    batch_size = len(cosine_sims)

                    for row_index in range(batch_size):
                        text_index = pre_batch_size * batch_index + row_index
                        for idx, doc_idx in enumerate(recalled_idx[row_index]):
                            # total_list.append([query_list[text_index], id2corpus[doc_idx], 1.0 - cosine_sims[row_index][idx]])
                            f.write(
                                "{}\t{}\t{}\n".format(
                                    query_list[text_index], self.id2corpus[doc_idx], 1.0-cosine_sims[row_index][idx]
                                )
                            )
            elif self.space == 'l2':
                for batch_index, batch_query_embedding in enumerate(query_total_embedding):
                    recalled_idx, l2_sims = self.index.knn_query(batch_query_embedding, recall_num)
                    batch_size = len(l2_sims)
                    for row_index in range(batch_size):
                        text_index = pre_batch_size * batch_index + row_index
                        for idx, doc_idx in enumerate(recalled_idx[row_index]):
                            f.write(
                                "{}\t{}\t{}\n".format(
                                    query_list[text_index], self.id2corpus[doc_idx], l2_sims[row_index][idx]
                                )
                            )

    '''向量召回,输入query_embedding,输出单个或几个召回结果'''
    def hnswlib_search(self,batch_query_embedding:list,query_list:list,recall_num):
        total_list = []
        if self.space == 'ip':
            recalled_idx, cosine_sims = self.index.knn_query(batch_query_embedding, recall_num)
            batch_size = len(cosine_sims)
            for row_index in range(batch_size):
                for idx, doc_idx in enumerate(recalled_idx[row_index]):
                    total_list.append([query_list[row_index], self.id2corpus[doc_idx], 1-cosine_sims[row_index][idx]])
        elif self.space == 'l2':
            for batch_index, batch_query_embedding in enumerate(query_total_embedding):
                recalled_idx, l2_sims = self.index.knn_query(batch_query_embedding, recall_num)
                batch_size = len(l2_sims)
                for row_index in range(batch_size):
                    for idx, doc_idx in enumerate(recalled_idx[row_index]):
                        total_list.append([query_list[row_index], self.id2corpus[doc_idx], l2_sims[row_index][idx]])
        print(total_list)
if __name__ == '__main__':

    pass



```



> 参考

* [《大规模向量相似度计算(一)——hnswlib的基本使用示例》](https://blog.csdn.net/redhatforyou/article/details/106932293)
* [《大规模向量相似度计算(二)——hnswlib的参数含义》](https://blog.csdn.net/redhatforyou/article/details/107021560)
* [《HNSW的基本原理及使用》](https://blog.csdn.net/redhatforyou/article/details/109012660)



## Proxima

## vearch

## Jina













## 其他

* [几款多模态向量检索引擎：Faiss、milvus、Proxima、vearch、Jina等](https://zhuanlan.zhihu.com/p/364923722)

* [专业领域词汇的无监督挖掘](https://kexue.fm/archives/6540)