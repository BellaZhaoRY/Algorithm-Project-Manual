---
sort: 3
---


# 字词相似度比较


* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/文本匹配/字词相似度.html)

* [个人知乎](https://www.zhihu.com/people/zhangyj-n)


---
* 项目冷启动时没有标注数据
* 没有类似开源roformer sim的模型可以计算相似度
* 没有LLM提供服务

## 无监督

* average word vectors：简单的对句子中的所有词向量取平均，是一种简单有效的方法，
缺点：没有考虑到单词的顺序，只对15个字以内的短句子比较有效，丢掉了词与词间的相关意思，无法更精细的表达句子与句子之间的关系。
* tfidf-weighting word vectors：指对句子中的所有词向量根据tfidf权重加权求和，是常用的一种计算sentence embedding的方法，在某些问题上表现很好，相比于简单的对所有词向量求平均，考虑到了tfidf权重，因此句子中更重要的词占得比重就更大。
缺点：没有考虑到单词的顺序
* bag of words：这种方法对于短文本效果很差，对于长文本效果一般，通常在科研中用来做baseline。缺点：1.没有考虑到单词的顺序，2.忽略了单词的语义信息。
* LDA：计算出一片文档或者句子的主题分布。也常常用于文本分类任务
* smooth inverse frequency（简称SIF)为权重，对所有词的word vector加权平均，最后从中减掉principal component，得到sentence embedding
[1] Sanjeev Arora, et al. 2017. A Simple but Tough-to-Beat Baseline for Sentence Embeddings
* Word Mover’s Distance[2]（简称WMD），直接度量句子之间的相似度
[2] Matt J. Kusner, et al. 2015. From Word Embeddings To Document Distances
* LSI或LSA：LSI是处理相似度的，基于SVD分解，用于特征降维，LSI求解出来的相似度跟topic相关性很强，而句子结构等信息较少。顺便说下，句子中词的顺序是不会影响LSI相似度结果的。


* TF的定义
    * 有一个专门的术语来表示关键字出现的次数，叫“词频”(Term Frequency), 简写为TF。TF越大，通常相关性越高。但是，你可能会发现一个问题。 如果一篇小短文里出现了一次“Lucence”，而一部好几百页的书里提到两次“Lucence”，我们不会认为那部书与Lucence相关性更高。为了消除文档本身大小的影响，一般使用TF时会把文本长度考虑上：
    `TF Score ＝ 某个词在文档中出现的次数 ／ 文档的长度`
   举例：某文档D，长度为200，其中“Lucence”出现了2次，“的”出现了20次，“原理”出现了3次，那么:
    `TF(Lucence|D) = 2/200 = 0.01`  
    `TF(的|D) = 20/200 = 0.1`
    `TF(原理|D) = 3/200 = 0.015`
 “Lucence的原理”这个短语与文档D的相关性就是三个词的相关性之和。
  `TF(Lucence的原理|D) = 0.01 + 0.1 + 0.015 = 0.125`
    * 我们发现一个问题，就是“的”这个词占了很大权重，而它对文档主题的几乎没什么贡献。这种词叫停用词，在度量相关性时不考虑它们的词频。去掉这个词后，上面的相关性变为0.025。其中“Lucence”贡献了0.01, “原理”贡献了0.015。
    细心的人还会发现，“原理”是个很通用的词，而“Lucence”是个专业词。直觉告诉我们，“Lucence”这个词对我们的搜索比“原理”更重要。抽象一下，可以理解为 一个词预测主题的能力越强，就越重要，权重也应该越大。反之，权重越小。
    * 假设我们把世界上所有的文档的总和看成一个文档库。如果一个词，很少在文档库里出现过，那通过它就容易找到目标，它的权重也应该大。反之，如果一个词在文档库中大量出现，看到它仍然不清楚在讲什么内容，它的权重就应该小。“的、地、得”这些虚词出现的频率太高，以至于权重设为零也不影响搜素，这也是它们成为停用词的原因之一。
* IDF的定义
    * 假设关键词w在n个文档中出现过，那么n越大，则w的权重越小。常用的方法叫“逆文本频率指数”(Inverse Dcument Frequency, 缩写为IDF)。一般的：`IDF = log(N/n)`
    注意: 这里的log是指以2为底的对数,不是以10为底的对数。
    N表示全部文档数。假如世界上文档总数位100亿，"Lucence"在1万个文档中出现过，“原理”在2亿个文档中出现过，那么它们的IDF值分别为：
    `IDF(Lucence) = log(100亿/1万) = 19.93`
    `IDF(原理) ＝ log(100亿/2亿) ＝ 5.64`
    * “Lucence”重要性相当于“原理”的3.5倍。停用词“的”在所有的文档里出现过，它的IDF=log(1)=0。短语与文档的最终相关性就是TF和IDF的加权求和：
    `simlarity = TF1*IDF1 + TF2*IDF2 + ... + TFn*IDFn`
    * 现在可以计算出上文中提到的“Lucence的原理”与文档D的相关性:
    `simlarity(Lucence的原理|D) = 0.01*19.93 + 0 + 5.64*0.015 ＝ 0.2839`
    其中，“Lucence”占了70%的权重，“原理”仅占30%的权重。

* Lucence中的TF-IDF
    * 早期的Lucence是直接把TF-IDF作为默认相似度来用的，只不过做了适当调整，它的相似度公式为:
    `simlarity = log(numDocs / (docFreq + 1)) * sqrt(tf) * (1/sqrt(length))`
    *  numDocs:索引中文档数量，对应前文中的N。lucence不是(也不可能)把整个互联网的文档作为基数，而是把索引中的文档总数作为基数。
    * docFreq: 包含关键字的文档数量，对应前文中的n。
    * tf: 关键字在文档中出现的次数。
    * length: 文档的长度。
    * 上面的公式在Lucence系统里做计算时会被拆分成三个部分：
    `IDF Score = log(numDocs / (docFreq + 1))`
    `TF Score = sqrt(tf)`
    `fieldNorms = 1/sqrt(length)`
    * fieldNorms 是对文本长度的归一化(Normalization)。所以，上面公式也可以表示成:
    `simlarity = IDF score * TF score * fieldNorms`

* BM25
    *  [BM25算法](https://zhuanlan.zhihu.com/p/79202151)
    *  [python根据BM25实现文本检索](https://www.jianshu.com/p/ff28004efb8a)
    *  [手动实现了Elasticsearch中的BM25](https://github.com/lsq960124/Inverted-index-BM25) 

## 参考

* [Learning to Rank](https://zhuanlan.zhihu.com/p/111636490)
* [【辩难】DSSM 损失函数是 Pointwise Loss 吗？](https://zhuanlan.zhihu.com/p/322065156)
