---
sort: 1
---

# 数据分布相关问题

> 持续更新中


[🔨算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98.html)



* [迁移学习中为什么要研究边缘概率分布和条件概率分布](https://www.zhihu.com/question/293820673)
    * 迁移学习的一大问题是领域自适应问题，即希望在有大量标注数据训练的源域中训练的模型，在只有少量标注或者无标注数据的目标域中有很好的泛化性。其中的挑战在于源域和目标域的数据分布很可能是不同的，即源域和目标域的联合概率分布可能是不同的。  
    * 由贝叶斯公式可将联合概率计算分为边缘概率分布和条件概率分布。
    * 条件概率分布相同，也就是学习任务相同，但源域和目标域的输入边缘分布可能不同，或是标签分布可能不同。输入边缘分布不同，我们设计一个统一的标签，使得映射一致；标签分布不同，我们使数据分布尽量均衡




## 专业概念

* 协变量    
    *  协变量是不在受控制（研究范围）的自变量内，但仍会影响研究结果的变量。
    *  研究某种自变量对因变量的影响，则实验过程中除研究的自变量因变量之外，还有其他很多变量对实验造成影响，而这些其他变量中，可以被控制的叫控制变量，不可被控制的叫协变量
* 联合概率
    *  包含多个条件且所有条件同时成立的概率，记作P(X=a,Y=b)或P(a,b)，有的书上也习惯记作P(ab)
* 边缘概率
    *  边缘概率是与联合概率对应的，P(X=a)或P(Y=b)，这类仅与单个随机变量有关的概率称为边缘概率
* 条件概率
    *  条件概率表示在条件Y=b成立的情况下，X=a的概率，记作P(X=a\|Y=b)或P(a\|b)
* 贝叶斯公式
    *  先验概率：知道原因推结果的，P(原因)、P(结果\|原因)等
    *  后验概率：根据结果推原因的，P(原因\|结果)等
    *  贝叶斯公式解决的是一些原因X无法直接观测、测量，而我们希望通过其结果Y来反推出原因X的问题，也就是知道一部分先验概率，来求后验概率的问题。
* element-wise
    *  快速逐元素element-wise product = element-wise multiplication = Hadamard product 两个矩阵对应位置元素进行乘积
    




## 参考链接

* [在迁移学习中，边缘概率分布和条件概率分布有什么含义？](https://www.zhihu.com/question/293820673)
* [详解深度学习中的Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
* [深度学习中 Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762/answer/85238569)
* [全连接层的作用是什么？](https://www.zhihu.com/question/41037974/answer/150522307)
* [numpy的数组和向量化计算](https://pydata.readthedocs.io/zh/latest/chapters/p04_numpy_basics_arrays_and_vectorized_computation.html)
