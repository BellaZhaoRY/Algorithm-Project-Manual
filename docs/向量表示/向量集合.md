---
sort: 1
---

# 向量集合

* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/向量表示/向量集合.html)

* [个人知乎](https://www.zhihu.com/people/zhangyj-n)

[TOC]

## 向量库

### Chinese Word Vectors 

* <a href="https://github.com/Embedding/Chinese-Word-Vectors/blob/master/README_zh.md">Chinese Word Vectors 中文词向量</a>

下列词向量基于不同的表示方式、不同的上下文特征以及不同领域的语料训练而成。

<table align="center">
    <tr align="center">
        <td colspan="5"><b>Word2vec / Skip-Gram with Negative Sampling (SGNS)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1IiIbQGJ_AooTj5s8aZYcvA">300d</a> / PWD: 5555</td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1AmXYWVgkxrG4GokevPtNgA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZKePwxwsDdzNrfkc6WKdGQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZBVVD4mUSUuXOxlZ3V71ZA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19wQrclyynOnco3JBvnI5pA">300d</td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/19sqMz-JAhhxh3o6ecvQxQw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1upPkA8KJnxTZBfjuNDtaeQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1BvKk2QjbtQMch7EISppW2A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19Vso_k79FZb5OZCWQPAnFQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13yVrXeGYkxdGW3P6juiQmA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1pUqyn7mnPcUmzxT64gGpSw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/1EhtsbDa3ekzZPODWNLHcXA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FcPHv7S4vUgnL7WeWf4_PA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13CAxY5ffRFuOcHZu8VmArw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sqvrUtGBAZ7YWEsGz41DRQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VGOs0RH7DXE5vRrtw6boQA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1OQ6fQLCgqT43WTwh5fh_lg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1_xogqF9kJT6tmQHSAYrYeg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Fo27Lv_0nz8FXg-xbOz14Q">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/1zbuUJEEEpZRNHxZ7Gezzmw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/11PWBcvruXEDvKf2TiIXntg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/10bhJpaXMCUK02nHvRAttqA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FHl_bQkYucvVk-j2KG4dxA">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1ciq8iXtcrHpu3ir_VhK0zg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Oa4CkPd8o2xd6LEAaa4gmg">300d</a> / PWD: z5b4</td>
      <td><a href="https://pan.baidu.com/s/1IG8IxNp2s7vVklz-vyZR9A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1SEOKrJYS14HpqIaQT462kA">300d</a> / PWD: yenb</td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1vPSeUsSiWYXEWAuokLR0qQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sS9E7sclvS_UZcBgHN7xLQ">300d</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合<br>Baidu Netdisk / Google Drive</td>
      <td>
        <a href="https://pan.baidu.com/s/1luy-GlTdqqvJ3j-A4FcIOw">300d</a><br>
        <a href="https://drive.google.com/open?id=1Zh9ZCEu8_eSQ-qkYVQufQDNKPC4mtEKR">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1oJol-GaRMk4-8Ejpzxo6Gw">300d</a><br>
        <a href="https://drive.google.com/open?id=1WUU9LnoAjs--1E_WqcghLJ-Pp8bb38oS">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1DjIGENlhRbsVyHW-caRePg">300d</a><br>
        <a href="https://drive.google.com/open?id=1aVAK0Z2E5DkdIH6-JHbiWSL5dbAcz6c3">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/14JP1gD7hcmsWdSpTvA3vKA">300d</a><br>
        <a href="https://drive.google.com/open?id=1kSAl4_AOg3_6ayU7KRM0Nk66uGdSZdnk">300d</a>
      </td>
    </tr>


<table align="center">
    <tr align="center">
        <td colspan="5"><b>Positive Pointwise Mutual Information (PPMI)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1_itcjrQawCwcURa7WZLPOA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1cEZzN1S2senwWSyHOnL7YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1KcfFdyO0-kE9S9CwzIisfw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FXYM3CY161_4QMgiH8vasQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1MGXRrc54nITPzQ7sfEUjMA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1mtxZna8UJ7xBIxhBFntumQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dDImpAx41V73Byl2julOGA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1bsBQHXFpxMHGBexYof1_rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/1NLr1K7aapU2sYBvzbVny5g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1LJl3Br0ccGDHP0XX2k3pVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1GQQXGMn1AHh-BlifT0JD2g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1Xm9Ec3O3rJ6ayrwVwonC7g">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1ECA51CZLp9_JB_me7YZ9-Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FO39ZYy1mStERf_b53Y_yQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1lLBFBk8nn3spFAvKY9IJ6A">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1f-dLQZlZo_-B5ZKcPIc6rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/10wtgdmrTsTrjpSDvI0KzOw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b6zjvhOIqTdACSSbriisVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1w24vCfgqcoJvPxsB5VrRvw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b9BPiDRhiEZ-6ybTcovrqQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VaUP3YJC0IZKTbJ-1_8HZg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1g39PKwT0kSmpneKOgXR5YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1d8Bsuak0fyXxQOVUiNr-2w">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1D5fteBX0Vy4czEqpxXjlrQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/15O2EbToOzjNSkzJwAOk_Ug">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/11Dqywn0hfMhysto7bZS1Dw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1wY-7mfV6nwDj_tru6W9h4Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1DMW-MgLApbQnWwDd-pT_qw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1HTHhlr8zvzhTwed7dO0sDg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1jAuGJBxKqgapt__urGsBOQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/173AJfCoAV0ZA8Z31tKBdTA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dFCxke_Su3lLsuwZr7co3A">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1NJ1Gc99oE0-GV0QxBqy-qw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1YGEgyXIbw0O4NtoM1ohjdA">Sparse</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
    </tr>

由于古汉语中绝大部份词均为单字词，因此只需字向量。

**不同的上下文共现信息**

我们提供了基于不同共现信息训练而成的词向量。下述提到的中心向量和上下文向量在类似的论文中也被称为输入和输出向量。

这个部分中的向量不仅仅是词向量，还有其它的语言单位对应的向量。比如，在上下文是“词-字”的条件下，上下文向量会包含字向量。

所有的向量均采用SGNS在百度百科语料上训练而成。

<table align="center">
  <tr align="center">
    <td><b>特征</b></td>
    <td><b>共现信息</b></td>
    <td><b>中心向量</b></td>
    <td><b>上下文向量</b></td>
  </tr>
  <tr align="center">
  	<td rowspan="1">词</td>
    <td>词 → 词</td>
    <td><a href="https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg">300d</a></td>
 	  <td><a href="https://pan.baidu.com/s/18T6DRVmS_cZu5u64EbbESQ">300d</a></td>
  </tr>
   <tr align="center">
    <td rowspan="3">N元组</td>
    <td>词 → N元组 (1-2)</td>
    <td><a href="https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw">300d</a></td>
 	  <td><a href="https://pan.baidu.com/s/12asujjAaaqxNFYRNP-MThw">300d</a></td>
  </tr>
  <tr align="center">
    <td>词 → N元组 (1-3)</td>
    <td><a href="https://pan.baidu.com/s/1oUmbxsnSuXf2jU8Jxu7U8A">300d</a></td>
 	  <td><a href="https://pan.baidu.com/s/1ylg6FfFHa0kXbiVz8bIL8g">300d</a></td>
  </tr>
  <tr align="center">
    <td>N元组 (1-2) → N元组 (1-2)</td>
    <td><a href="https://pan.baidu.com/s/1Za7DIGVhE6dMsTmxHb-izg">300d</a></td>
 	  <td><a href="https://pan.baidu.com/s/1oKI4Cs9eo7bg5mqfY1hdmg">300d</a></td>
  </tr>
  <tr align="center">
    <td rowspan="3">字</td>
    <td>词 → 字 (1)</td>
 	  <td><a href="https://pan.baidu.com/s/1c9yiosHKNIZwRlLzD_F1ig">300d</a></td>
    <td><a href="https://pan.baidu.com/s/1KGZ_x8r-lq-AuElLCSVzvQ">300d</a></td>
  </tr>
  <tr align="center">
    <td>词 → 字 (1-2)</td>
 	  <td><a href="https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw">300d</a></td>
    <td><a href="https://pan.baidu.com/s/1q0ItLzbn5Tfb3LhepRCeEA">300d</a></td>
  </tr>
  <tr align="center">
    <td>词 → 字 (1-4)</td>
    <td><a href="https://pan.baidu.com/s/1WNWAnba56Rqjmx-FAN_7_g">300d</a></td>
 	  <td><a href="https://pan.baidu.com/s/1hJKTAz6PwS7wmz9wQgmYeg">300d</a></td>
  </tr>
  <tr align="center">
  	<td rowspan="1">偏旁部首</td>
    <td>偏旁部首</td>
    <td>300d</td>
 	  <td>300d</td>
  </tr>
  <tr align="center">
    <td rowspan="2">位置</td>
    <td>词 → 词 (左/右)</td>
    <td><a href="https://pan.baidu.com/s/1JvjcrXFZPknT5H5Xw6KRVg">300d</a></td>
 	  <td><a href="https://pan.baidu.com/s/1m6K9CnIIS8FrQZdDuF6hPQ">300d</a></td>
  </tr>
  <tr align="center">
    <td>词 → 词 (距离)</td>
    <td><a href="https://pan.baidu.com/s/1c29BDu4R1hyUX-sgvlHJnA">300d</a></td>
 	  <td><a href="https://pan.baidu.com/s/1sMZHIc-7eU6gRalHwtBHZw">300d</a></td>
  </tr>
  <tr align="center">
    <td>全局信息</td>
    <td>词 → 文章</td>
    <td>300d</td>
 	  <td>300d</td>
  </tr>
  <tr align="center">
    <td rowspan="2">语法特征</td>
    <td>词 → 词性</td>
    <td>300d</td>
 	  <td>300d</td>
  </tr>
  <tr align="center">
    <td>词 → 依存关系</td>
    <td>300d</td>
 	  <td>300d</td>
  </tr>
</table>


* 训练方案

  * 训练基本参数

    * | **窗口大小** | **动态窗口** | **子采样** | **低频词阈值** | **迭代次数** | **负采样\*** |
      | ------------ | ------------ | ---------- | -------------- | ------------ | ------------ |
      | 5            | 是           | 1e-5       | 10             | 5            | 5            |

    * 使用[ngram2vec](https://github.com/zhezhaoa/ngram2vec/)训练

  * 表示方式

    * 稠密词向量 SGNS
      * 通过浅层神经网络训练而成的一种低维实向量来表示词语。它通常也被称之为神经词嵌入方法

    * 稀疏词向量PPMI
      * 基于正值逐点互信息并且以稀疏方式表示的特征汇总模型

  * 上下文特征

    * 在词向量领域通常有三种主要的上下文特征：**词**、**N元组**、**字**。==大部分词表示方式本质上都是在利用词和词的共现统计信息==，换句话说就是把词来当作上下文特征，即上文**词特征**中的向量。受到语言模型的启发，我们将N元组信息也引入了上下文特征中。不只是词和词的共现信息，词和N元组的共现信息也被用在了上文**N元组特征**向量的训练中。对于中文来说，汉字承载了较强的含义，因此我们利用了词和词的共现信息以及词和字的共现信息来训练了一些向量。在上文**字特征**中的向量包括了长度1至4的字级别N元组。
    * 除了词、N元组和字以外。还有一些特征对词向量的性质有重要的影响。比如，把**整篇文本**作为特征引入训练中会使得**词向量受到文章主题**的影响；用**依存分析**的结果作为上下文特征来训练词向量会让**词向量受到语法**的影响。本项目共涉及17种不同的共现信息。

  * 语料

    * 多领域,移除html和xml标记,保留纯文本,使用hanlp分词,繁体转简体



> 关键论文
>
> [ngram2vec](https://github.com/zhezhaoa/ngram2vec/)
>
> [SGNS-skip-gram with negative-sample](https://blog.csdn.net/rosefun96/article/details/108413273)
>
> [PPMI](https://blog.csdn.net/qq_43186282/article/details/115009348)



### fastNLP

<a href="https://docs.qq.com/sheet/DVnpkTnF6VW9UeXdh?tab=BB08J2&_t=1676561011336">fastNLP可加载embedding与数据集</a>

<img src="https://note.youdao.com/yws/api/personal/file/WEBc4a78e7f5e022ea57bf91e47666ca7ac?method=download&shareKey=c41c2157c57d1af80d1d0249709f52e4" />



### Text2vec


* <a href="https://github.com/shibing624/text2vec">Text2vec:Text to Vector</a>

<table align="center">
  <tr align="center">
    <td><b>Arch</b></td>
    <td><b>BaseModel</b></td>
    <td><b>Model</b></td>
  </tr>
   <tr align="center">
  	<td rowspan="1">Word2Vec</td>
    <td>word2vec</td>
    <td><a href="https://ai.tencent.com/ailab/nlp/en/download.html">w2v-light-tencent-chinese</a></td>
  </tr>
  <tr align="center">
  	<td rowspan="1">SBERT</td>
    <td>xlm-roberta-base</td>
    <td><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2">sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</a></td>
  </tr>
   <tr align="center">
  	<td rowspan="1">CoSENT</td>
    <td>hfl/chinese-macbert-base	</td>
    <td><a href="https://huggingface.co/shibing624/text2vec-base-chinese">	shibing624/text2vec-base-chinese</a></td>
  </tr>
   <tr align="center">
  	<td rowspan="1">CoSENT</td>
    <td>hfl/chinese-lert-large</td>
    <td><a href="https://huggingface.co/GanymedeNil/text2vec-large-chinese">GanymedeNil/text2vec-large-chinese</a></td>
  </tr>
   <tr align="center">
  	<td rowspan="1">CoSENT</td>
    <td>nghuyong/ernie-3.0-base-zh</td>
    <td><a href="https://huggingface.co/shibing624/text2vec-base-chinese-sentence">shibing624/text2vec-base-chinese-sentence</a></td>
  </tr>
   <tr align="center">
  	<td rowspan="1">CoSENT</td>
    <td>nghuyong/ernie-3.0-base-zh</td>
    <td><a href="https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase">shibing624/text2vec-base-chinese-paraphrase</a></td>
  </tr>
   <tr align="center">
  	<td rowspan="1">CoSENT</td>
    <td>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</td>
    <td><a href="https://huggingface.co/shibing624/text2vec-base-multilingual">shibing624/text2vec-base-multilingual
</a></td>
  </tr>
</table>  


* 介绍
  * 基础预训练模型: bert,roberta,macbert
  * 采用的训练方法基于cosent
  * 评价指标:spearman系数



> 关键论文: 科学空间站
>
> [CoSENT（一）：比Sentence-BERT更有效的句向量方案](https://kexue.fm/archives/8847)
>
> [CoSENT（二）：特征式匹配与交互式匹配有多大差距？](https://kexue.fm/archives/8860)
>
> [CoSENT（三）：作为交互式相似度的损失函数](https://kexue.fm/archives/9341)
>
> [从局部到全局：语义相似度的测地线距离](https://kexue.fm/archives/9368)
>
> [利用CUR分解加速交互式相似度模型的检索](https://kexue.fm/archives/9336)



### uniem

* <a href="https://github.com/wangyuxinwhy/uniem">uniem</a>

* 介绍
  * M3E 是 Moka Massive Mixed Embedding 的缩写
  * Moka，此模型由 MokaAI 训练，开源和评测，训练脚本使用 [uniem](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/train_m3e.py) ，评测 BenchMark 使用 [MTEB-zh](https://github.com/wangyuxinwhy/uniem/tree/main/mteb-zh)
  * Massive，此模型通过**千万级** (2200w+) 的中文句对数据集进行训练
  * Mixed，此模型支持中英双语的同质文本相似度计算，异质文本检索等功能，未来还会支持代码检索
  * Embedding，此模型是文本嵌入模型，可以将自然语言转换成稠密的向量
    * s2s, 即 sentence to sentence ，代表了同质文本之间的嵌入能力，适用任务：文本相似度，重复问题检测，文本分类等
    * s2p, 即 sentence to passage ，代表了异质文本之间的嵌入能力，适用任务：文本检索，GPT 记忆模块等
    * s2c, 即 sentence to code ，代表了自然语言和程序语言之间的嵌入能力，适用任务：代码检索

* 训练方案
  * 预训练模型roberta
  * 微调:in-batch 负采样,使用 A100 80G 来最大化 batch-size，并在共计 2200W+ 的句对数据集上训练了 1 epoch。训练脚本使用 [uniem](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/train_m3e.py)
  
* 数据
  * 中文训练集，M3E 在大规模句对数据集上的训练，包含中文百科，金融，医疗，法律，新闻，学术等多个领域共计 2200W 句对样本，数据集详见 [M3E 数据集](https://huggingface.co/moka-ai/m3e-base/blob/main/README.md#M3E数据集)
  * 英文训练集，M3E 使用 MEDI 145W 英文三元组数据集进行训练，数据集详见 [MEDI 数据集](https://drive.google.com/file/d/1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52/view)，此数据集由 [instructor team](https://github.com/HKUNLP/instructor-embedding) 提供
  * 指令数据集，M3E 使用了 300W + 的指令微调数据集，这使得 M3E 对文本编码的时候可以遵从指令，这部分的工作主要被启发于 [instructor-embedding](https://github.com/HKUNLP/instructor-embedding)
  * 基础模型，M3E 使用 hfl 实验室的 [Roberta](https://huggingface.co/hfl/chinese-roberta-wwm-ext) 系列模型进行训练，目前提供 small 和 base 两个版本，大家则需选用
  * ALL IN ONE，M3E 旨在提供一个 ALL IN ONE 的文本嵌入模型，不仅支持同质句子相似度判断，还支持异质文本检索，你只需要一个模型就可以覆盖全部的应用场景，未来还会支持代码检索

* 微调教程
  * [嵌入模型微调](https://github.com/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb)  (给的示例都是有监督的)
  * 支持的数据
    * PairRecord  句子对
    * TripletRecord 三元组
    * ScoredPairRecord 带分数句对

  * 支持SGPT， 使用 GPT 系列模型（transformer decoder）作为 Embedding 模型的基础模型
  * Embedding 向量的提取策略不再是 LastMeanPooling ，而是根据 token position 来加权平均
  * 使用 bitfit 的微调策略，在微调时只对模型的 bias 进行更新

* 评测标准
  * 中文评测标准 MTEB-zh,不同任务评测的不同,分类任务采用Accuracy,检索排序任务采用 map@1, map@10, mrr@1, mrr@10, ndcg@1, ndcg@10


> 关键论文   
>
> [instructor-embedding](https://github.com/HKUNLP/instructor-embedding)
>
> [SGPT](https://github.com/Muennighoff/sgpt)   https://blog.csdn.net/Simonsdu/article/details/123757260



### FlagEmbedding


* <a href="https://github.com/FlagOpen/FlagEmbedding">FlagEmbedding</a>

* 训练方案


  * 我们按照 [retromae](https://github.com/staoxiao/RetroMAE) 方法对模型进行**预训练**， 其在检索任务中表现出了良好的性能( [参考论文](https://aclanthology.org/2022.emnlp-main.35.pdf) )。 预训练是在24块A100(40G) gpu上进行的，batch大小为720。在retromae中，编码器和解码器的掩码率分别为0.3和0.5。 使用AdamW优化器，学习率为2e-5

  * 预训练数据使用悟道

  * 微调采用对比学习,数据使用三元组和in-batch负样本.采用跨设备负样本共享方法在不同的gpu间共享负样本,在48块A100(40G) gpu上训练模型，batch大小为32,768。 我们使用AdamW优化器，学习率为1e-5。 对比损失的温度系数为0.01

  * 针对s2p添加了指令数据,微调数据对于英语，我们从 [wikipedia](https://huggingface.co/datasets/wikipedia) ， [cc-net](https://github.com/facebookresearch/cc_net) 等收集了2.3亿个文本对。 -对于中文，我们从 [悟道](https://github.com/BAAI-WuDao/Data) 、[simclue](https://github.com/CLUEbenchmark/SimCLUE)等收集了1.2亿对文本。

* 注意问题


  * 不相似句子之间的相似度分数很高

    * 由于采取了对比学习进行训练且温度系数设置为0.01, 当前BGE模型的相似度分布并不是[0, 1]区间的均匀分布，其大概处于[0.6, 1]这个区间，因此并不是大于0.6就代表相似。 尤其是对于长度较短句子之间的相似度，当前模型的相似度数值会偏高。
    * 对于检索任务或者相似度任务，影响结果的是不同句子间相似度的相对大小关系，而不是绝对数值。** 如若需要根据相似度阈值筛选相似句子，请根据实际数据上的相似度分布情况，使用一个合适的相似度阈值（如0.8, 0.85，甚至是0.9）。 如果你想调整相似度分布，你可以在你的数据上使用更大的温度系数微调模型或者尝试使用其他损失函数。 
  *  什么时候需要添加查询指令

    * 对于一个使用短查询寻找相关长文档的检索任务，查询与文档之间长度非常不一致，推荐为短查询添加指令。 其他任务，推荐不添加指令，例如，像quora这类用一个较短的问题去搜索其他相关的短问题，推荐不添加指令。 具体是否添加指令，可以根据实际情况选择其中表现最好的方式。 在所有情况下，文档端都不用添加指令，只是查询端可以选择是否添加指令。




> 关键论文 
>
> [RetroMAE](https://github.com/staoxiao/RetroMAE)



下一篇 [相关论文阅读](https://kg-nlp.github.io/Algorithm-Project-Manual/向量表示/相关论文阅读.html)























